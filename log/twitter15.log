E:\Programming\Miniconda3\python.exe D:/Project/RumorDetection/run.py
task:  twitter15
WARNING:root:The number of nodes in your data object can only be inferred by its edge indices, and hence may result in unexpected batch-wise behavior, e.g., in case there exists isolated nodes. Please consider explicitly setting the number of nodes for this data object by assigning it to data.num_nodes.
WARNING:root:The number of nodes in your data object can only be inferred by its edge indices, and hence may result in unexpected batch-wise behavior, e.g., in case there exists isolated nodes. Please consider explicitly setting the number of nodes for this data object by assigning it to data.num_nodes.
#nodes:  4459
GLAN(
  (loss_func): CrossEntropyLoss()
  (word_embedding): Embedding(11696, 300, padding_idx=0)
  (user_tweet_embedding): Embedding(4459, 300, padding_idx=0)
  (mh_attention): MultiheadAttention(
    (linear1): Linear(in_features=300, out_features=300, bias=True)
    (linear2): Linear(in_features=300, out_features=300, bias=True)
    (linear3): Linear(in_features=300, out_features=300, bias=True)
    (dropout): Dropout(p=0.0, inplace=False)
  )
  (linear_fuse): Linear(in_features=600, out_features=1, bias=True)
  (gnn1): GATConv(300, 8, heads=8)
  (gnn2): GATConv(64, 300, heads=1)
  (attention): Attention(
    (linear1): Linear(in_features=600, out_features=300, bias=True)
    (linear2): Linear(in_features=300, out_features=1, bias=True)
    (activation): ReLU()
    (dropout): Dropout(p=0.5, inplace=False)
  )
  (convs_source): ModuleList(
    (0): Conv1d(300, 100, kernel_size=(3,), stride=(1,))
    (1): Conv1d(300, 100, kernel_size=(4,), stride=(1,))
    (2): Conv1d(300, 100, kernel_size=(5,), stride=(1,))
  )
  (convs_replies): ModuleList(
    (0): Conv1d(300, 100, kernel_size=(3,), stride=(1,))
    (1): Conv1d(300, 100, kernel_size=(4,), stride=(1,))
    (2): Conv1d(300, 100, kernel_size=(5,), stride=(1,))
  )
  (max_poolings): ModuleList(
    (0): MaxPool1d(kernel_size=48, stride=48, padding=0, dilation=1, ceil_mode=False)
    (1): MaxPool1d(kernel_size=47, stride=47, padding=0, dilation=1, ceil_mode=False)
    (2): MaxPool1d(kernel_size=46, stride=46, padding=0, dilation=1, ceil_mode=False)
  )
  (dropout): Dropout(p=0.5, inplace=False)
  (relu): ReLU()
  (fc_out): Sequential(
    (0): Linear(in_features=600, out_features=300, bias=True)
    (1): Dropout(p=0.5, inplace=False)
    (2): ReLU()
    (3): Linear(in_features=300, out_features=4, bias=True)
  )
)

Epoch  1 / 30
Batch[0] - loss: 1.421252  acc: 18.7500%(3/16)
Batch[1] - loss: 1.417219  acc: 18.7500%(3/16)
Batch[2] - loss: 1.378256  acc: 25.0000%(4/16)
Batch[3] - loss: 1.495700  acc: 25.0000%(4/16)
Batch[4] - loss: 1.369747  acc: 25.0000%(4/16)
Batch[5] - loss: 1.407723  acc: 25.0000%(4/16)
Batch[6] - loss: 1.377290  acc: 37.5000%(6/16)
Batch[7] - loss: 1.378976  acc: 31.2500%(5/16)
Batch[8] - loss: 1.441391  acc: 18.7500%(3/16)
Batch[9] - loss: 1.340817  acc: 31.2500%(5/16)
Batch[10] - loss: 1.371461  acc: 37.5000%(6/16)
Batch[11] - loss: 1.408113  acc: 18.7500%(3/16)
Batch[12] - loss: 1.408946  acc: 18.7500%(3/16)
Batch[13] - loss: 1.407688  acc: 25.0000%(4/16)
Batch[14] - loss: 1.390864  acc: 25.0000%(4/16)
Batch[15] - loss: 1.306019  acc: 31.2500%(5/16)
Batch[16] - loss: 1.363384  acc: 25.0000%(4/16)
Batch[17] - loss: 1.250420  acc: 31.2500%(5/16)
Batch[18] - loss: 1.372627  acc: 31.2500%(5/16)
Batch[19] - loss: 1.320063  acc: 50.0000%(8/16)
Batch[20] - loss: 1.344299  acc: 31.2500%(5/16)
Batch[21] - loss: 1.407414  acc: 25.0000%(4/16)
Batch[22] - loss: 1.523543  acc: 18.7500%(3/16)
Batch[23] - loss: 1.393966  acc: 12.5000%(2/16)
Batch[24] - loss: 1.324870  acc: 31.2500%(5/16)
Batch[25] - loss: 1.351347  acc: 37.5000%(6/16)
Batch[26] - loss: 1.284928  acc: 56.2500%(9/16)
Batch[27] - loss: 1.361871  acc: 37.5000%(6/16)
Batch[28] - loss: 1.488997  acc: 6.2500%(1/16)
Batch[29] - loss: 1.326281  acc: 18.7500%(3/16)
Batch[30] - loss: 1.303197  acc: 37.5000%(6/16)
Batch[31] - loss: 1.387944  acc: 25.0000%(4/16)
Batch[32] - loss: 1.412039  acc: 18.7500%(3/16)
Batch[33] - loss: 1.342801  acc: 31.2500%(5/16)
Batch[34] - loss: 1.442027  acc: 18.7500%(3/16)
Batch[35] - loss: 1.360161  acc: 37.5000%(6/16)
Batch[36] - loss: 1.390432  acc: 37.5000%(6/16)
Batch[37] - loss: 1.284515  acc: 37.5000%(6/16)
Batch[38] - loss: 1.299770  acc: 31.2500%(5/16)
Batch[39] - loss: 1.498185  acc: 25.0000%(4/16)
Batch[40] - loss: 1.396374  acc: 31.2500%(5/16)
Batch[41] - loss: 1.440586  acc: 25.0000%(4/16)
Batch[42] - loss: 1.384421  acc: 25.0000%(4/16)
Batch[43] - loss: 1.503360  acc: 12.5000%(2/16)
Batch[44] - loss: 1.339661  acc: 31.2500%(5/16)
Batch[45] - loss: 1.465276  acc: 25.0000%(4/16)
Batch[46] - loss: 1.399649  acc: 31.2500%(5/16)
Batch[47] - loss: 1.439250  acc: 18.7500%(3/16)
Batch[48] - loss: 1.295176  acc: 31.2500%(5/16)
Batch[49] - loss: 1.269149  acc: 50.0000%(8/16)
Batch[50] - loss: 1.459152  acc: 18.7500%(3/16)
Batch[51] - loss: 1.367034  acc: 18.7500%(3/16)
Batch[52] - loss: 1.515333  acc: 12.5000%(2/16)
Batch[53] - loss: 1.371938  acc: 31.2500%(5/16)
Batch[54] - loss: 1.395424  acc: 31.2500%(5/16)
Batch[55] - loss: 1.297327  acc: 31.2500%(5/16)
Batch[56] - loss: 1.299089  acc: 43.7500%(7/16)
Batch[57] - loss: 1.457640  acc: 31.2500%(5/16)
Batch[58] - loss: 1.438493  acc: 50.0000%(8/16)
Batch[59] - loss: 1.375666  acc: 43.7500%(7/16)
Batch[60] - loss: 1.333447  acc: 18.7500%(3/16)
Batch[61] - loss: 1.380048  acc: 37.5000%(6/16)
Batch[62] - loss: 1.261722  acc: 30.7692%(4/13)
Average loss:1.381615 average acc:28.663006%
              precision    recall  f1-score   support

          NR    0.28571   0.73684   0.41176        38
          FR    0.17143   0.16216   0.16667        37
          UR    0.25000   0.05405   0.08889        37
          TR    0.75000   0.16216   0.26667        37

    accuracy                        0.28188       149
   macro avg    0.36429   0.27881   0.23350       149
weighted avg    0.36376   0.28188   0.23469       149

Val set acc: 0.28187919463087246
Best val set acc: 0.28187919463087246
save model!!!

Epoch  2 / 30
Batch[0] - loss: 1.318077  acc: 25.0000%(4/16)
Batch[1] - loss: 1.368802  acc: 31.2500%(5/16)
Batch[2] - loss: 1.363352  acc: 37.5000%(6/16)
Batch[3] - loss: 1.277815  acc: 31.2500%(5/16)
Batch[4] - loss: 1.406019  acc: 18.7500%(3/16)
Batch[5] - loss: 1.214415  acc: 43.7500%(7/16)
Batch[6] - loss: 1.457216  acc: 12.5000%(2/16)
Batch[7] - loss: 1.450833  acc: 31.2500%(5/16)
Batch[8] - loss: 1.278077  acc: 50.0000%(8/16)
Batch[9] - loss: 1.336096  acc: 37.5000%(6/16)
Batch[10] - loss: 1.237736  acc: 50.0000%(8/16)
Batch[11] - loss: 1.207956  acc: 56.2500%(9/16)
Batch[12] - loss: 1.206072  acc: 62.5000%(10/16)
Batch[13] - loss: 1.178569  acc: 50.0000%(8/16)
Batch[14] - loss: 1.184283  acc: 50.0000%(8/16)
Batch[15] - loss: 1.080740  acc: 37.5000%(6/16)
Batch[16] - loss: 1.300477  acc: 50.0000%(8/16)
Batch[17] - loss: 1.137850  acc: 56.2500%(9/16)
Batch[18] - loss: 1.263671  acc: 50.0000%(8/16)
Batch[19] - loss: 1.406403  acc: 37.5000%(6/16)
Batch[20] - loss: 1.290460  acc: 31.2500%(5/16)
Batch[21] - loss: 1.401433  acc: 37.5000%(6/16)
Batch[22] - loss: 1.250731  acc: 37.5000%(6/16)
Batch[23] - loss: 1.237770  acc: 50.0000%(8/16)
Batch[24] - loss: 1.394045  acc: 37.5000%(6/16)
Batch[25] - loss: 1.294909  acc: 37.5000%(6/16)
Batch[26] - loss: 1.243099  acc: 37.5000%(6/16)
Batch[27] - loss: 1.416383  acc: 31.2500%(5/16)
Batch[28] - loss: 1.274695  acc: 37.5000%(6/16)
Batch[29] - loss: 1.253135  acc: 50.0000%(8/16)
Batch[30] - loss: 1.337355  acc: 43.7500%(7/16)
Batch[31] - loss: 1.416696  acc: 12.5000%(2/16)
Batch[32] - loss: 1.308723  acc: 18.7500%(3/16)
Batch[33] - loss: 1.327340  acc: 25.0000%(4/16)
Batch[34] - loss: 1.161533  acc: 56.2500%(9/16)
Batch[35] - loss: 1.320769  acc: 50.0000%(8/16)
Batch[36] - loss: 1.202781  acc: 43.7500%(7/16)
Batch[37] - loss: 1.306519  acc: 37.5000%(6/16)
Batch[38] - loss: 1.293259  acc: 31.2500%(5/16)
Batch[39] - loss: 1.172433  acc: 37.5000%(6/16)
Batch[40] - loss: 1.158512  acc: 50.0000%(8/16)
Batch[41] - loss: 1.102584  acc: 68.7500%(11/16)
Batch[42] - loss: 1.195660  acc: 43.7500%(7/16)
Batch[43] - loss: 1.130970  acc: 43.7500%(7/16)
Batch[44] - loss: 0.999234  acc: 81.2500%(13/16)
Batch[45] - loss: 1.319588  acc: 31.2500%(5/16)
Batch[46] - loss: 1.305030  acc: 37.5000%(6/16)
Batch[47] - loss: 1.292449  acc: 31.2500%(5/16)
Batch[48] - loss: 1.254464  acc: 50.0000%(8/16)
Batch[49] - loss: 1.393500  acc: 31.2500%(5/16)
Batch[50] - loss: 1.276698  acc: 37.5000%(6/16)
Batch[51] - loss: 1.317977  acc: 31.2500%(5/16)
Batch[52] - loss: 1.235155  acc: 31.2500%(5/16)
Batch[53] - loss: 1.089389  acc: 43.7500%(7/16)
Batch[54] - loss: 1.338312  acc: 18.7500%(3/16)
Batch[55] - loss: 1.157372  acc: 62.5000%(10/16)
Batch[56] - loss: 1.291545  acc: 56.2500%(9/16)
Batch[57] - loss: 1.092487  acc: 62.5000%(10/16)
Batch[58] - loss: 1.501781  acc: 43.7500%(7/16)
Batch[59] - loss: 0.940487  acc: 68.7500%(11/16)
Batch[60] - loss: 0.961464  acc: 68.7500%(11/16)
Batch[61] - loss: 0.983418  acc: 68.7500%(11/16)
Batch[62] - loss: 1.612793  acc: 15.3846%(2/13)
Average loss:1.262371 average acc:41.910870%
              precision    recall  f1-score   support

          NR    0.36364   0.84211   0.50794        38
          FR    0.75000   0.16216   0.26667        37
          UR    0.48649   0.48649   0.48649        37
          TR    0.75000   0.32432   0.45283        37

    accuracy                        0.45638       149
   macro avg    0.58753   0.45377   0.42848       149
weighted avg    0.58603   0.45638   0.42901       149

Val set acc: 0.4563758389261745
Best val set acc: 0.4563758389261745
save model!!!

Epoch  3 / 30
Batch[0] - loss: 0.991334  acc: 56.2500%(9/16)
Batch[1] - loss: 1.530212  acc: 43.7500%(7/16)
Batch[2] - loss: 1.011140  acc: 68.7500%(11/16)
Batch[3] - loss: 1.047838  acc: 56.2500%(9/16)
Batch[4] - loss: 1.002245  acc: 56.2500%(9/16)
Batch[5] - loss: 0.954362  acc: 56.2500%(9/16)
Batch[6] - loss: 0.729186  acc: 81.2500%(13/16)
Batch[7] - loss: 1.031979  acc: 68.7500%(11/16)
Batch[8] - loss: 1.014883  acc: 50.0000%(8/16)
Batch[9] - loss: 0.801773  acc: 68.7500%(11/16)
Batch[10] - loss: 0.974942  acc: 56.2500%(9/16)
Batch[11] - loss: 1.170324  acc: 37.5000%(6/16)
Batch[12] - loss: 0.775433  acc: 62.5000%(10/16)
Batch[13] - loss: 0.897597  acc: 68.7500%(11/16)
Batch[14] - loss: 0.809001  acc: 75.0000%(12/16)
Batch[15] - loss: 0.687193  acc: 68.7500%(11/16)
Batch[16] - loss: 0.835134  acc: 62.5000%(10/16)
Batch[17] - loss: 1.183295  acc: 50.0000%(8/16)
Batch[18] - loss: 1.070180  acc: 50.0000%(8/16)
Batch[19] - loss: 0.616330  acc: 75.0000%(12/16)
Batch[20] - loss: 1.318574  acc: 56.2500%(9/16)
Batch[21] - loss: 0.713128  acc: 81.2500%(13/16)
Batch[22] - loss: 0.988230  acc: 50.0000%(8/16)
Batch[23] - loss: 0.698505  acc: 75.0000%(12/16)
Batch[24] - loss: 0.682730  acc: 75.0000%(12/16)
Batch[25] - loss: 0.843517  acc: 68.7500%(11/16)
Batch[26] - loss: 0.989738  acc: 56.2500%(9/16)
Batch[27] - loss: 0.837201  acc: 68.7500%(11/16)
Batch[28] - loss: 0.603768  acc: 87.5000%(14/16)
Batch[29] - loss: 0.744775  acc: 62.5000%(10/16)
Batch[30] - loss: 0.710972  acc: 81.2500%(13/16)
Batch[31] - loss: 1.900935  acc: 43.7500%(7/16)
Batch[32] - loss: 0.729806  acc: 68.7500%(11/16)
Batch[33] - loss: 0.675979  acc: 81.2500%(13/16)
Batch[34] - loss: 0.857024  acc: 50.0000%(8/16)
Batch[35] - loss: 0.812389  acc: 56.2500%(9/16)
Batch[36] - loss: 1.058481  acc: 62.5000%(10/16)
Batch[37] - loss: 0.857434  acc: 81.2500%(13/16)
Batch[38] - loss: 0.947633  acc: 62.5000%(10/16)
Batch[39] - loss: 0.879146  acc: 81.2500%(13/16)
Batch[40] - loss: 0.942147  acc: 62.5000%(10/16)
Batch[41] - loss: 0.640032  acc: 75.0000%(12/16)
Batch[42] - loss: 0.601223  acc: 75.0000%(12/16)
Batch[43] - loss: 0.965709  acc: 68.7500%(11/16)
Batch[44] - loss: 0.820153  acc: 68.7500%(11/16)
Batch[45] - loss: 0.818473  acc: 68.7500%(11/16)
Batch[46] - loss: 0.763815  acc: 68.7500%(11/16)
Batch[47] - loss: 0.852122  acc: 75.0000%(12/16)
Batch[48] - loss: 1.229770  acc: 50.0000%(8/16)
Batch[49] - loss: 1.157155  acc: 50.0000%(8/16)
Batch[50] - loss: 0.877104  acc: 81.2500%(13/16)
Batch[51] - loss: 0.939132  acc: 62.5000%(10/16)
Batch[52] - loss: 0.650240  acc: 75.0000%(12/16)
Batch[53] - loss: 0.807909  acc: 56.2500%(9/16)
Batch[54] - loss: 0.823196  acc: 68.7500%(11/16)
Batch[55] - loss: 0.939406  acc: 68.7500%(11/16)
Batch[56] - loss: 0.446453  acc: 87.5000%(14/16)
Batch[57] - loss: 1.020381  acc: 62.5000%(10/16)
Batch[58] - loss: 0.849018  acc: 62.5000%(10/16)
Batch[59] - loss: 0.740680  acc: 81.2500%(13/16)
Batch[60] - loss: 1.021886  acc: 68.7500%(11/16)
Batch[61] - loss: 0.756285  acc: 68.7500%(11/16)
Batch[62] - loss: 0.949514  acc: 76.9231%(10/13)
Average loss:0.898352 average acc:65.804337%
              precision    recall  f1-score   support

          NR    0.88571   0.81579   0.84932        38
          FR    0.60000   0.64865   0.62338        37
          UR    0.57143   0.64865   0.60759        37
          TR    0.93750   0.81081   0.86957        37

    accuracy                        0.73154       149
   macro avg    0.74866   0.73097   0.73746       149
weighted avg    0.74958   0.73154   0.73821       149

Val set acc: 0.7315436241610739
Best val set acc: 0.7315436241610739
save model!!!

Epoch  4 / 30
Batch[0] - loss: 0.666875  acc: 68.7500%(11/16)
Batch[1] - loss: 0.633509  acc: 81.2500%(13/16)
Batch[2] - loss: 0.629995  acc: 75.0000%(12/16)
Batch[3] - loss: 0.834833  acc: 68.7500%(11/16)
Batch[4] - loss: 0.567683  acc: 81.2500%(13/16)
Batch[5] - loss: 0.632391  acc: 75.0000%(12/16)
Batch[6] - loss: 0.671338  acc: 75.0000%(12/16)
Batch[7] - loss: 0.435399  acc: 93.7500%(15/16)
Batch[8] - loss: 0.470034  acc: 93.7500%(15/16)
Batch[9] - loss: 0.635433  acc: 75.0000%(12/16)
Batch[10] - loss: 0.659755  acc: 75.0000%(12/16)
Batch[11] - loss: 0.477768  acc: 87.5000%(14/16)
Batch[12] - loss: 0.441745  acc: 93.7500%(15/16)
Batch[13] - loss: 0.436069  acc: 87.5000%(14/16)
Batch[14] - loss: 0.591918  acc: 62.5000%(10/16)
Batch[15] - loss: 0.591526  acc: 81.2500%(13/16)
Batch[16] - loss: 0.474375  acc: 81.2500%(13/16)
Batch[17] - loss: 0.364963  acc: 87.5000%(14/16)
Batch[18] - loss: 0.314632  acc: 100.0000%(16/16)
Batch[19] - loss: 0.742979  acc: 62.5000%(10/16)
Batch[20] - loss: 0.769221  acc: 75.0000%(12/16)
Batch[21] - loss: 0.439226  acc: 81.2500%(13/16)
Batch[22] - loss: 0.527055  acc: 87.5000%(14/16)
Batch[23] - loss: 0.829733  acc: 68.7500%(11/16)
Batch[24] - loss: 0.361053  acc: 87.5000%(14/16)
Batch[25] - loss: 0.354488  acc: 93.7500%(15/16)
Batch[26] - loss: 0.459149  acc: 87.5000%(14/16)
Batch[27] - loss: 0.949423  acc: 62.5000%(10/16)
Batch[28] - loss: 0.390978  acc: 93.7500%(15/16)
Batch[29] - loss: 0.524399  acc: 81.2500%(13/16)
Batch[30] - loss: 0.545330  acc: 87.5000%(14/16)
Batch[31] - loss: 0.503626  acc: 81.2500%(13/16)
Batch[32] - loss: 0.529977  acc: 81.2500%(13/16)
Batch[33] - loss: 0.416011  acc: 87.5000%(14/16)
Batch[34] - loss: 0.231352  acc: 100.0000%(16/16)
Batch[35] - loss: 0.289027  acc: 93.7500%(15/16)
Batch[36] - loss: 0.286858  acc: 93.7500%(15/16)
Batch[37] - loss: 0.644578  acc: 87.5000%(14/16)
Batch[38] - loss: 0.402499  acc: 81.2500%(13/16)
Batch[39] - loss: 0.240978  acc: 100.0000%(16/16)
Batch[40] - loss: 0.385372  acc: 87.5000%(14/16)
Batch[41] - loss: 0.327253  acc: 93.7500%(15/16)
Batch[42] - loss: 0.640402  acc: 81.2500%(13/16)
Batch[43] - loss: 0.394481  acc: 81.2500%(13/16)
Batch[44] - loss: 0.682296  acc: 75.0000%(12/16)
Batch[45] - loss: 0.239479  acc: 93.7500%(15/16)
Batch[46] - loss: 0.340910  acc: 87.5000%(14/16)
Batch[47] - loss: 0.422843  acc: 93.7500%(15/16)
Batch[48] - loss: 0.656729  acc: 81.2500%(13/16)
Batch[49] - loss: 1.234054  acc: 62.5000%(10/16)
Batch[50] - loss: 0.460639  acc: 87.5000%(14/16)
Batch[51] - loss: 0.288918  acc: 87.5000%(14/16)
Batch[52] - loss: 0.566731  acc: 87.5000%(14/16)
Batch[53] - loss: 0.303857  acc: 93.7500%(15/16)
Batch[54] - loss: 0.401873  acc: 87.5000%(14/16)
Batch[55] - loss: 0.541227  acc: 75.0000%(12/16)
Batch[56] - loss: 0.332781  acc: 93.7500%(15/16)
Batch[57] - loss: 0.661452  acc: 87.5000%(14/16)
Batch[58] - loss: 0.423106  acc: 75.0000%(12/16)
Batch[59] - loss: 0.454449  acc: 81.2500%(13/16)
Batch[60] - loss: 0.356087  acc: 87.5000%(14/16)
Batch[61] - loss: 0.302825  acc: 87.5000%(14/16)
Batch[62] - loss: 0.794436  acc: 61.5385%(8/13)
Average loss:0.510800 average acc:83.318077%
              precision    recall  f1-score   support

          NR    0.90000   0.71053   0.79412        38
          FR    0.70588   0.64865   0.67606        37
          UR    0.53571   0.81081   0.64516        37
          TR    0.96552   0.75676   0.84848        37

    accuracy                        0.73154       149
   macro avg    0.77678   0.73169   0.74096       149
weighted avg    0.77761   0.73154   0.74131       149


Epoch  5 / 30
Batch[0] - loss: 0.701566  acc: 87.5000%(14/16)
Batch[1] - loss: 0.172330  acc: 100.0000%(16/16)
Batch[2] - loss: 0.470062  acc: 87.5000%(14/16)
Batch[3] - loss: 0.241796  acc: 87.5000%(14/16)
Batch[4] - loss: 0.138645  acc: 100.0000%(16/16)
Batch[5] - loss: 0.188473  acc: 100.0000%(16/16)
Batch[6] - loss: 0.320572  acc: 87.5000%(14/16)
Batch[7] - loss: 0.125547  acc: 100.0000%(16/16)
Batch[8] - loss: 0.206985  acc: 93.7500%(15/16)
Batch[9] - loss: 0.266031  acc: 87.5000%(14/16)
Batch[10] - loss: 0.331321  acc: 87.5000%(14/16)
Batch[11] - loss: 0.118091  acc: 93.7500%(15/16)
Batch[12] - loss: 0.197197  acc: 93.7500%(15/16)
Batch[13] - loss: 0.132616  acc: 100.0000%(16/16)
Batch[14] - loss: 0.272063  acc: 87.5000%(14/16)
Batch[15] - loss: 0.242123  acc: 93.7500%(15/16)
Batch[16] - loss: 0.312990  acc: 87.5000%(14/16)
Batch[17] - loss: 0.132555  acc: 100.0000%(16/16)
Batch[18] - loss: 0.088613  acc: 100.0000%(16/16)
Batch[19] - loss: 0.148860  acc: 100.0000%(16/16)
Batch[20] - loss: 0.217367  acc: 100.0000%(16/16)
Batch[21] - loss: 0.277736  acc: 87.5000%(14/16)
Batch[22] - loss: 0.207067  acc: 93.7500%(15/16)
Batch[23] - loss: 0.213857  acc: 100.0000%(16/16)
Batch[24] - loss: 0.990546  acc: 87.5000%(14/16)
Batch[25] - loss: 0.186608  acc: 100.0000%(16/16)
Batch[26] - loss: 0.291004  acc: 93.7500%(15/16)
Batch[27] - loss: 0.214402  acc: 87.5000%(14/16)
Batch[28] - loss: 0.167018  acc: 87.5000%(14/16)
Batch[29] - loss: 0.341530  acc: 87.5000%(14/16)
Batch[30] - loss: 0.136533  acc: 93.7500%(15/16)
Batch[31] - loss: 0.171623  acc: 100.0000%(16/16)
Batch[32] - loss: 0.186083  acc: 87.5000%(14/16)
Batch[33] - loss: 0.296749  acc: 87.5000%(14/16)
Batch[34] - loss: 0.169631  acc: 100.0000%(16/16)
Batch[35] - loss: 0.060429  acc: 100.0000%(16/16)
Batch[36] - loss: 0.089266  acc: 100.0000%(16/16)
Batch[37] - loss: 0.161342  acc: 93.7500%(15/16)
Batch[38] - loss: 0.305616  acc: 87.5000%(14/16)
Batch[39] - loss: 0.561750  acc: 81.2500%(13/16)
Batch[40] - loss: 0.140583  acc: 93.7500%(15/16)
Batch[41] - loss: 0.206591  acc: 93.7500%(15/16)
Batch[42] - loss: 0.154482  acc: 93.7500%(15/16)
Batch[43] - loss: 0.220682  acc: 87.5000%(14/16)
Batch[44] - loss: 0.057813  acc: 100.0000%(16/16)
Batch[45] - loss: 0.736908  acc: 87.5000%(14/16)
Batch[46] - loss: 0.128802  acc: 100.0000%(16/16)
Batch[47] - loss: 0.071562  acc: 100.0000%(16/16)
Batch[48] - loss: 0.445721  acc: 81.2500%(13/16)
Batch[49] - loss: 0.124716  acc: 93.7500%(15/16)
Batch[50] - loss: 0.079337  acc: 100.0000%(16/16)
Batch[51] - loss: 0.126683  acc: 100.0000%(16/16)
Batch[52] - loss: 0.542646  acc: 87.5000%(14/16)
Batch[53] - loss: 0.504456  acc: 81.2500%(13/16)
Batch[54] - loss: 0.061847  acc: 100.0000%(16/16)
Batch[55] - loss: 0.104528  acc: 93.7500%(15/16)
Batch[56] - loss: 0.116431  acc: 100.0000%(16/16)
Batch[57] - loss: 0.317130  acc: 93.7500%(15/16)
Batch[58] - loss: 0.196087  acc: 93.7500%(15/16)
Batch[59] - loss: 0.416026  acc: 87.5000%(14/16)
Batch[60] - loss: 0.263452  acc: 93.7500%(15/16)
Batch[61] - loss: 1.084587  acc: 75.0000%(12/16)
Batch[62] - loss: 0.093691  acc: 100.0000%(13/13)
Average loss:0.257926 average acc:93.154770%
              precision    recall  f1-score   support

          NR    0.87879   0.76316   0.81690        38
          FR    0.95833   0.62162   0.75410        37
          UR    0.57143   0.86486   0.68817        37
          TR    0.83333   0.81081   0.82192        37

    accuracy                        0.76510       149
   macro avg    0.81047   0.76511   0.77027       149
weighted avg    0.81093   0.76510   0.77059       149

Val set acc: 0.7651006711409396
Best val set acc: 0.7651006711409396
save model!!!

Epoch  6 / 30
Batch[0] - loss: 0.074901  acc: 100.0000%(16/16)
Batch[1] - loss: 0.126885  acc: 100.0000%(16/16)
Batch[2] - loss: 0.045704  acc: 100.0000%(16/16)
Batch[3] - loss: 0.049083  acc: 100.0000%(16/16)
Batch[4] - loss: 0.051745  acc: 100.0000%(16/16)
Batch[5] - loss: 0.201056  acc: 87.5000%(14/16)
Batch[6] - loss: 0.043797  acc: 100.0000%(16/16)
Batch[7] - loss: 0.052748  acc: 100.0000%(16/16)
Batch[8] - loss: 0.115607  acc: 93.7500%(15/16)
Batch[9] - loss: 0.091420  acc: 93.7500%(15/16)
Batch[10] - loss: 0.127273  acc: 100.0000%(16/16)
Batch[11] - loss: 0.061653  acc: 100.0000%(16/16)
Batch[12] - loss: 0.040450  acc: 100.0000%(16/16)
Batch[13] - loss: 0.176478  acc: 93.7500%(15/16)
Batch[14] - loss: 0.091229  acc: 100.0000%(16/16)
Batch[15] - loss: 0.385186  acc: 93.7500%(15/16)
Batch[16] - loss: 0.283513  acc: 87.5000%(14/16)
Batch[17] - loss: 0.027452  acc: 100.0000%(16/16)
Batch[18] - loss: 0.122192  acc: 93.7500%(15/16)
Batch[19] - loss: 0.053150  acc: 100.0000%(16/16)
Batch[20] - loss: 0.123792  acc: 93.7500%(15/16)
Batch[21] - loss: 0.197525  acc: 93.7500%(15/16)
Batch[22] - loss: 0.085715  acc: 100.0000%(16/16)
Batch[23] - loss: 0.245996  acc: 93.7500%(15/16)
Batch[24] - loss: 0.109772  acc: 100.0000%(16/16)
Batch[25] - loss: 0.057265  acc: 100.0000%(16/16)
Batch[26] - loss: 0.169609  acc: 93.7500%(15/16)
Batch[27] - loss: 0.111151  acc: 93.7500%(15/16)
Batch[28] - loss: 0.051694  acc: 100.0000%(16/16)
Batch[29] - loss: 0.073185  acc: 100.0000%(16/16)
Batch[30] - loss: 0.174717  acc: 93.7500%(15/16)
Batch[31] - loss: 0.235971  acc: 87.5000%(14/16)
Batch[32] - loss: 0.173888  acc: 93.7500%(15/16)
Batch[33] - loss: 0.182465  acc: 93.7500%(15/16)
Batch[34] - loss: 0.188890  acc: 87.5000%(14/16)
Batch[35] - loss: 0.204210  acc: 87.5000%(14/16)
Batch[36] - loss: 0.131405  acc: 93.7500%(15/16)
Batch[37] - loss: 0.053876  acc: 100.0000%(16/16)
Batch[38] - loss: 0.253302  acc: 93.7500%(15/16)
Batch[39] - loss: 0.084488  acc: 100.0000%(16/16)
Batch[40] - loss: 0.150694  acc: 93.7500%(15/16)
Batch[41] - loss: 0.058246  acc: 100.0000%(16/16)
Batch[42] - loss: 0.047733  acc: 100.0000%(16/16)
Batch[43] - loss: 0.233756  acc: 93.7500%(15/16)
Batch[44] - loss: 0.150053  acc: 93.7500%(15/16)
Batch[45] - loss: 0.056769  acc: 100.0000%(16/16)
Batch[46] - loss: 0.133640  acc: 100.0000%(16/16)
Batch[47] - loss: 0.117025  acc: 93.7500%(15/16)
Batch[48] - loss: 0.112080  acc: 93.7500%(15/16)
Batch[49] - loss: 0.049691  acc: 100.0000%(16/16)
Batch[50] - loss: 0.088334  acc: 100.0000%(16/16)
Batch[51] - loss: 0.138269  acc: 93.7500%(15/16)
Batch[52] - loss: 0.165066  acc: 93.7500%(15/16)
Batch[53] - loss: 0.043409  acc: 100.0000%(16/16)
Batch[54] - loss: 0.096222  acc: 93.7500%(15/16)
Batch[55] - loss: 0.031899  acc: 100.0000%(16/16)
Batch[56] - loss: 0.122404  acc: 93.7500%(15/16)
Batch[57] - loss: 0.091900  acc: 93.7500%(15/16)
Batch[58] - loss: 0.044088  acc: 100.0000%(16/16)
Batch[59] - loss: 0.033189  acc: 100.0000%(16/16)
Batch[60] - loss: 0.131920  acc: 100.0000%(16/16)
Batch[61] - loss: 0.013955  acc: 100.0000%(16/16)
Batch[62] - loss: 0.204752  acc: 92.3077%(12/13)
Average loss:0.118183 average acc:96.405685%
              precision    recall  f1-score   support

          NR    0.94118   0.84211   0.88889        38
          FR    0.80556   0.78378   0.79452        37
          UR    0.68182   0.81081   0.74074        37
          TR    0.85714   0.81081   0.83333        37

    accuracy                        0.81208       149
   macro avg    0.82142   0.81188   0.81437       149
weighted avg    0.82223   0.81208   0.81487       149

Val set acc: 0.8120805369127517
Best val set acc: 0.8120805369127517
save model!!!

Epoch  7 / 30
Batch[0] - loss: 0.011355  acc: 100.0000%(16/16)
Batch[1] - loss: 0.008549  acc: 100.0000%(16/16)
Batch[2] - loss: 0.008313  acc: 100.0000%(16/16)
Batch[3] - loss: 0.016854  acc: 100.0000%(16/16)
Batch[4] - loss: 0.027385  acc: 100.0000%(16/16)
Batch[5] - loss: 0.022872  acc: 100.0000%(16/16)
Batch[6] - loss: 0.059186  acc: 100.0000%(16/16)
Batch[7] - loss: 0.009986  acc: 100.0000%(16/16)
Batch[8] - loss: 0.058033  acc: 100.0000%(16/16)
Batch[9] - loss: 0.043917  acc: 100.0000%(16/16)
Batch[10] - loss: 0.187726  acc: 87.5000%(14/16)
Batch[11] - loss: 0.078965  acc: 100.0000%(16/16)
Batch[12] - loss: 0.124513  acc: 93.7500%(15/16)
Batch[13] - loss: 0.073067  acc: 100.0000%(16/16)
Batch[14] - loss: 0.014627  acc: 100.0000%(16/16)
Batch[15] - loss: 0.091886  acc: 93.7500%(15/16)
Batch[16] - loss: 0.017369  acc: 100.0000%(16/16)
Batch[17] - loss: 0.033246  acc: 100.0000%(16/16)
Batch[18] - loss: 0.028796  acc: 100.0000%(16/16)
Batch[19] - loss: 0.148566  acc: 87.5000%(14/16)
Batch[20] - loss: 0.460665  acc: 75.0000%(12/16)
Batch[21] - loss: 0.144577  acc: 93.7500%(15/16)
Batch[22] - loss: 0.042853  acc: 100.0000%(16/16)
Batch[23] - loss: 0.662562  acc: 81.2500%(13/16)
Batch[24] - loss: 0.018180  acc: 100.0000%(16/16)
Batch[25] - loss: 0.307130  acc: 93.7500%(15/16)
Batch[26] - loss: 0.007709  acc: 100.0000%(16/16)
Batch[27] - loss: 0.025053  acc: 100.0000%(16/16)
Batch[28] - loss: 0.018230  acc: 100.0000%(16/16)
Batch[29] - loss: 0.022167  acc: 100.0000%(16/16)
Batch[30] - loss: 0.007769  acc: 100.0000%(16/16)
Batch[31] - loss: 0.052503  acc: 100.0000%(16/16)
Batch[32] - loss: 0.025816  acc: 100.0000%(16/16)
Batch[33] - loss: 0.099065  acc: 100.0000%(16/16)
Batch[34] - loss: 0.022272  acc: 100.0000%(16/16)
Batch[35] - loss: 0.058299  acc: 100.0000%(16/16)
Batch[36] - loss: 0.026338  acc: 100.0000%(16/16)
Batch[37] - loss: 0.057814  acc: 100.0000%(16/16)
Batch[38] - loss: 0.124240  acc: 93.7500%(15/16)
Batch[39] - loss: 0.053744  acc: 100.0000%(16/16)
Batch[40] - loss: 0.169786  acc: 93.7500%(15/16)
Batch[41] - loss: 0.049802  acc: 100.0000%(16/16)
Batch[42] - loss: 0.067113  acc: 93.7500%(15/16)
Batch[43] - loss: 0.017094  acc: 100.0000%(16/16)
Batch[44] - loss: 0.079677  acc: 93.7500%(15/16)
Batch[45] - loss: 0.243780  acc: 93.7500%(15/16)
Batch[46] - loss: 0.071137  acc: 100.0000%(16/16)
Batch[47] - loss: 0.032407  acc: 100.0000%(16/16)
Batch[48] - loss: 0.037404  acc: 100.0000%(16/16)
Batch[49] - loss: 0.021986  acc: 100.0000%(16/16)
Batch[50] - loss: 0.069533  acc: 100.0000%(16/16)
Batch[51] - loss: 0.044796  acc: 100.0000%(16/16)
Batch[52] - loss: 0.090196  acc: 93.7500%(15/16)
Batch[53] - loss: 0.040798  acc: 100.0000%(16/16)
Batch[54] - loss: 0.005878  acc: 100.0000%(16/16)
Batch[55] - loss: 0.028557  acc: 100.0000%(16/16)
Batch[56] - loss: 0.074064  acc: 93.7500%(15/16)
Batch[57] - loss: 0.086679  acc: 93.7500%(15/16)
Batch[58] - loss: 0.013724  acc: 100.0000%(16/16)
Batch[59] - loss: 0.015892  acc: 100.0000%(16/16)
Batch[60] - loss: 0.023382  acc: 100.0000%(16/16)
Batch[61] - loss: 0.002772  acc: 100.0000%(16/16)
Batch[62] - loss: 0.022396  acc: 100.0000%(13/13)
Average loss:0.073191 average acc:97.718262%
              precision    recall  f1-score   support

          NR    0.90625   0.76316   0.82857        38
          FR    0.82353   0.75676   0.78873        37
          UR    0.63265   0.83784   0.72093        37
          TR    0.85294   0.78378   0.81690        37

    accuracy                        0.78523       149
   macro avg    0.80384   0.78538   0.78878       149
weighted avg    0.80453   0.78523   0.78905       149


Epoch  8 / 30
Batch[0] - loss: 0.026498  acc: 100.0000%(16/16)
Batch[1] - loss: 0.038685  acc: 100.0000%(16/16)
Batch[2] - loss: 0.137663  acc: 87.5000%(14/16)
Batch[3] - loss: 0.008299  acc: 100.0000%(16/16)
Batch[4] - loss: 0.076295  acc: 100.0000%(16/16)
Batch[5] - loss: 0.019685  acc: 100.0000%(16/16)
Batch[6] - loss: 0.081926  acc: 100.0000%(16/16)
Batch[7] - loss: 0.070183  acc: 100.0000%(16/16)
Batch[8] - loss: 0.198163  acc: 93.7500%(15/16)
Batch[9] - loss: 0.041746  acc: 100.0000%(16/16)
Batch[10] - loss: 0.016271  acc: 100.0000%(16/16)
Batch[11] - loss: 0.028161  acc: 100.0000%(16/16)
Batch[12] - loss: 0.029119  acc: 100.0000%(16/16)
Batch[13] - loss: 0.008821  acc: 100.0000%(16/16)
Batch[14] - loss: 0.004369  acc: 100.0000%(16/16)
Batch[15] - loss: 0.009547  acc: 100.0000%(16/16)
Batch[16] - loss: 0.034212  acc: 100.0000%(16/16)
Batch[17] - loss: 0.024985  acc: 100.0000%(16/16)
Batch[18] - loss: 0.084764  acc: 93.7500%(15/16)
Batch[19] - loss: 0.012097  acc: 100.0000%(16/16)
Batch[20] - loss: 0.002458  acc: 100.0000%(16/16)
Batch[21] - loss: 0.160331  acc: 93.7500%(15/16)
Batch[22] - loss: 0.025730  acc: 100.0000%(16/16)
Batch[23] - loss: 0.010260  acc: 100.0000%(16/16)
Batch[24] - loss: 0.040732  acc: 100.0000%(16/16)
Batch[25] - loss: 0.015836  acc: 100.0000%(16/16)
Batch[26] - loss: 0.491461  acc: 81.2500%(13/16)
Batch[27] - loss: 0.115107  acc: 93.7500%(15/16)
Batch[28] - loss: 0.006808  acc: 100.0000%(16/16)
Batch[29] - loss: 0.020023  acc: 100.0000%(16/16)
Batch[30] - loss: 0.069953  acc: 100.0000%(16/16)
Batch[31] - loss: 0.002670  acc: 100.0000%(16/16)
Batch[32] - loss: 0.005095  acc: 100.0000%(16/16)
Batch[33] - loss: 0.054262  acc: 100.0000%(16/16)
Batch[34] - loss: 0.190534  acc: 93.7500%(15/16)
Batch[35] - loss: 0.301787  acc: 87.5000%(14/16)
Batch[36] - loss: 0.033805  acc: 100.0000%(16/16)
Batch[37] - loss: 0.003172  acc: 100.0000%(16/16)
Batch[38] - loss: 0.010794  acc: 100.0000%(16/16)
Batch[39] - loss: 0.083176  acc: 100.0000%(16/16)
Batch[40] - loss: 0.005939  acc: 100.0000%(16/16)
Batch[41] - loss: 0.029793  acc: 100.0000%(16/16)
Batch[42] - loss: 0.016310  acc: 100.0000%(16/16)
Batch[43] - loss: 0.060642  acc: 93.7500%(15/16)
Batch[44] - loss: 0.012675  acc: 100.0000%(16/16)
Batch[45] - loss: 0.009794  acc: 100.0000%(16/16)
Batch[46] - loss: 0.010543  acc: 100.0000%(16/16)
Batch[47] - loss: 0.012755  acc: 100.0000%(16/16)
Batch[48] - loss: 0.012514  acc: 100.0000%(16/16)
Batch[49] - loss: 0.097410  acc: 93.7500%(15/16)
Batch[50] - loss: 0.001065  acc: 100.0000%(16/16)
Batch[51] - loss: 0.009686  acc: 100.0000%(16/16)
Batch[52] - loss: 0.026220  acc: 100.0000%(16/16)
Batch[53] - loss: 0.008277  acc: 100.0000%(16/16)
Batch[54] - loss: 0.118160  acc: 93.7500%(15/16)
Batch[55] - loss: 0.178663  acc: 93.7500%(15/16)
Batch[56] - loss: 0.002605  acc: 100.0000%(16/16)
Batch[57] - loss: 0.196854  acc: 93.7500%(15/16)
Batch[58] - loss: 0.011968  acc: 100.0000%(16/16)
Batch[59] - loss: 0.008362  acc: 100.0000%(16/16)
Batch[60] - loss: 0.085209  acc: 93.7500%(15/16)
Batch[61] - loss: 0.070545  acc: 93.7500%(15/16)
Batch[62] - loss: 0.214165  acc: 92.3077%(12/13)
Average loss:0.060248 average acc:97.992981%
              precision    recall  f1-score   support

          NR    0.90909   0.78947   0.84507        38
          FR    0.90323   0.75676   0.82353        37
          UR    0.62500   0.81081   0.70588        37
          TR    0.83784   0.83784   0.83784        37

    accuracy                        0.79866       149
   macro avg    0.81879   0.79872   0.80308       149
weighted avg    0.81939   0.79866   0.80336       149


Epoch  9 / 30
Batch[0] - loss: 0.008691  acc: 100.0000%(16/16)
Batch[1] - loss: 0.037339  acc: 100.0000%(16/16)
Batch[2] - loss: 0.034345  acc: 100.0000%(16/16)
Batch[3] - loss: 0.148728  acc: 93.7500%(15/16)
Batch[4] - loss: 0.012404  acc: 100.0000%(16/16)
Batch[5] - loss: 0.013934  acc: 100.0000%(16/16)
Batch[6] - loss: 0.305787  acc: 87.5000%(14/16)
Batch[7] - loss: 0.030334  acc: 100.0000%(16/16)
Batch[8] - loss: 0.030180  acc: 100.0000%(16/16)
Batch[9] - loss: 0.049023  acc: 100.0000%(16/16)
Batch[10] - loss: 0.010641  acc: 100.0000%(16/16)
Batch[11] - loss: 0.026813  acc: 100.0000%(16/16)
Batch[12] - loss: 0.092246  acc: 93.7500%(15/16)
Batch[13] - loss: 0.035840  acc: 100.0000%(16/16)
Batch[14] - loss: 0.005724  acc: 100.0000%(16/16)
Batch[15] - loss: 0.023733  acc: 100.0000%(16/16)
Batch[16] - loss: 0.007640  acc: 100.0000%(16/16)
Batch[17] - loss: 0.002489  acc: 100.0000%(16/16)
Batch[18] - loss: 0.021495  acc: 100.0000%(16/16)
Batch[19] - loss: 0.022883  acc: 100.0000%(16/16)
Batch[20] - loss: 0.137553  acc: 87.5000%(14/16)
Batch[21] - loss: 0.035983  acc: 100.0000%(16/16)
Batch[22] - loss: 0.040611  acc: 100.0000%(16/16)
Batch[23] - loss: 0.007945  acc: 100.0000%(16/16)
Batch[24] - loss: 0.015137  acc: 100.0000%(16/16)
Batch[25] - loss: 0.027102  acc: 100.0000%(16/16)
Batch[26] - loss: 0.177421  acc: 87.5000%(14/16)
Batch[27] - loss: 0.092659  acc: 100.0000%(16/16)
Batch[28] - loss: 0.058251  acc: 100.0000%(16/16)
Batch[29] - loss: 0.048182  acc: 100.0000%(16/16)
Batch[30] - loss: 0.022380  acc: 100.0000%(16/16)
Batch[31] - loss: 0.148559  acc: 93.7500%(15/16)
Batch[32] - loss: 0.134731  acc: 87.5000%(14/16)
Batch[33] - loss: 0.079184  acc: 93.7500%(15/16)
Batch[34] - loss: 0.028050  acc: 100.0000%(16/16)
Batch[35] - loss: 0.039941  acc: 100.0000%(16/16)
Batch[36] - loss: 0.002806  acc: 100.0000%(16/16)
Batch[37] - loss: 0.042714  acc: 100.0000%(16/16)
Batch[38] - loss: 0.037114  acc: 100.0000%(16/16)
Batch[39] - loss: 0.061103  acc: 93.7500%(15/16)
Batch[40] - loss: 0.005485  acc: 100.0000%(16/16)
Batch[41] - loss: 0.023169  acc: 100.0000%(16/16)
Batch[42] - loss: 0.037329  acc: 100.0000%(16/16)
Batch[43] - loss: 0.019681  acc: 100.0000%(16/16)
Batch[44] - loss: 0.005138  acc: 100.0000%(16/16)
Batch[45] - loss: 0.006227  acc: 100.0000%(16/16)
Batch[46] - loss: 0.085501  acc: 93.7500%(15/16)
Batch[47] - loss: 0.023537  acc: 100.0000%(16/16)
Batch[48] - loss: 0.009532  acc: 100.0000%(16/16)
Batch[49] - loss: 0.012900  acc: 100.0000%(16/16)
Batch[50] - loss: 0.005786  acc: 100.0000%(16/16)
Batch[51] - loss: 0.055894  acc: 100.0000%(16/16)
Batch[52] - loss: 0.004215  acc: 100.0000%(16/16)
Batch[53] - loss: 0.004299  acc: 100.0000%(16/16)
Batch[54] - loss: 0.024917  acc: 100.0000%(16/16)
Batch[55] - loss: 0.020289  acc: 100.0000%(16/16)
Batch[56] - loss: 0.102572  acc: 93.7500%(15/16)
Batch[57] - loss: 0.003301  acc: 100.0000%(16/16)
Batch[58] - loss: 0.000949  acc: 100.0000%(16/16)
Batch[59] - loss: 0.187654  acc: 93.7500%(15/16)
Batch[60] - loss: 0.003293  acc: 100.0000%(16/16)
Batch[61] - loss: 0.004205  acc: 100.0000%(16/16)
Batch[62] - loss: 0.014172  acc: 100.0000%(13/13)
Average loss:0.044726 average acc:98.412704%
              precision    recall  f1-score   support

          NR    0.76087   0.92105   0.83333        38
          FR    0.96296   0.70270   0.81250        37
          UR    0.72500   0.78378   0.75325        37
          TR    0.86111   0.83784   0.84932        37

    accuracy                        0.81208       149
   macro avg    0.82749   0.81134   0.81210       149
weighted avg    0.82704   0.81208   0.81224       149


Epoch  10 / 30
Batch[0] - loss: 0.108339  acc: 93.7500%(15/16)
Batch[1] - loss: 0.023440  acc: 100.0000%(16/16)
Batch[2] - loss: 0.055920  acc: 100.0000%(16/16)
Batch[3] - loss: 0.007649  acc: 100.0000%(16/16)
Batch[4] - loss: 0.026865  acc: 100.0000%(16/16)
Batch[5] - loss: 0.001603  acc: 100.0000%(16/16)
Batch[6] - loss: 0.026650  acc: 100.0000%(16/16)
Batch[7] - loss: 0.018307  acc: 100.0000%(16/16)
Batch[8] - loss: 0.002472  acc: 100.0000%(16/16)
Batch[9] - loss: 0.012970  acc: 100.0000%(16/16)
Batch[10] - loss: 0.052610  acc: 100.0000%(16/16)
Batch[11] - loss: 0.041020  acc: 100.0000%(16/16)
Batch[12] - loss: 0.027449  acc: 100.0000%(16/16)
Batch[13] - loss: 0.003294  acc: 100.0000%(16/16)
Batch[14] - loss: 0.007313  acc: 100.0000%(16/16)
Batch[15] - loss: 0.023457  acc: 100.0000%(16/16)
Batch[16] - loss: 0.044578  acc: 100.0000%(16/16)
Batch[17] - loss: 0.005032  acc: 100.0000%(16/16)
Batch[18] - loss: 0.008394  acc: 100.0000%(16/16)
Batch[19] - loss: 0.001326  acc: 100.0000%(16/16)
Batch[20] - loss: 0.149603  acc: 87.5000%(14/16)
Batch[21] - loss: 0.040536  acc: 100.0000%(16/16)
Batch[22] - loss: 0.002824  acc: 100.0000%(16/16)
Batch[23] - loss: 0.005238  acc: 100.0000%(16/16)
Batch[24] - loss: 0.076968  acc: 93.7500%(15/16)
Batch[25] - loss: 0.025152  acc: 100.0000%(16/16)
Batch[26] - loss: 0.164341  acc: 93.7500%(15/16)
Batch[27] - loss: 0.001204  acc: 100.0000%(16/16)
Batch[28] - loss: 0.033337  acc: 100.0000%(16/16)
Batch[29] - loss: 0.008169  acc: 100.0000%(16/16)
Batch[30] - loss: 0.006597  acc: 100.0000%(16/16)
Batch[31] - loss: 0.005660  acc: 100.0000%(16/16)
Batch[32] - loss: 0.002931  acc: 100.0000%(16/16)
Batch[33] - loss: 0.113654  acc: 93.7500%(15/16)
Batch[34] - loss: 0.010335  acc: 100.0000%(16/16)
Batch[35] - loss: 0.128124  acc: 100.0000%(16/16)
Batch[36] - loss: 0.013585  acc: 100.0000%(16/16)
Batch[37] - loss: 0.079669  acc: 93.7500%(15/16)
Batch[38] - loss: 0.010178  acc: 100.0000%(16/16)
Batch[39] - loss: 0.020666  acc: 100.0000%(16/16)
Batch[40] - loss: 0.022554  acc: 100.0000%(16/16)
Batch[41] - loss: 0.009711  acc: 100.0000%(16/16)
Batch[42] - loss: 0.035927  acc: 100.0000%(16/16)
Batch[43] - loss: 0.023066  acc: 100.0000%(16/16)
Batch[44] - loss: 0.091198  acc: 93.7500%(15/16)
Batch[45] - loss: 0.008708  acc: 100.0000%(16/16)
Batch[46] - loss: 0.990286  acc: 93.7500%(15/16)
Batch[47] - loss: 0.227380  acc: 93.7500%(15/16)
Batch[48] - loss: 0.000516  acc: 100.0000%(16/16)
Batch[49] - loss: 0.008550  acc: 100.0000%(16/16)
Batch[50] - loss: 0.052781  acc: 93.7500%(15/16)
Batch[51] - loss: 0.017480  acc: 100.0000%(16/16)
Batch[52] - loss: 0.004802  acc: 100.0000%(16/16)
Batch[53] - loss: 0.061689  acc: 93.7500%(15/16)
Batch[54] - loss: 0.005957  acc: 100.0000%(16/16)
Batch[55] - loss: 0.016584  acc: 100.0000%(16/16)
Batch[56] - loss: 0.023950  acc: 100.0000%(16/16)
Batch[57] - loss: 0.021592  acc: 100.0000%(16/16)
Batch[58] - loss: 0.094712  acc: 93.7500%(15/16)
Batch[59] - loss: 0.005234  acc: 100.0000%(16/16)
Batch[60] - loss: 0.009030  acc: 100.0000%(16/16)
Batch[61] - loss: 0.008237  acc: 100.0000%(16/16)
Batch[62] - loss: 0.040080  acc: 100.0000%(13/13)
Average loss:0.050436 average acc:98.710320%
              precision    recall  f1-score   support

          NR    0.89655   0.68421   0.77612        38
          FR    0.89286   0.67568   0.76923        37
          UR    0.58491   0.83784   0.68889        37
          TR    0.82051   0.86486   0.84211        37

    accuracy                        0.76510       149
   macro avg    0.79871   0.76565   0.76909       149
weighted avg    0.79936   0.76510   0.76913       149


Epoch  11 / 30
Batch[0] - loss: 0.004787  acc: 100.0000%(16/16)
Batch[1] - loss: 0.033692  acc: 100.0000%(16/16)
Batch[2] - loss: 0.023288  acc: 100.0000%(16/16)
Batch[3] - loss: 0.168564  acc: 93.7500%(15/16)
Batch[4] - loss: 0.045446  acc: 100.0000%(16/16)
Batch[5] - loss: 0.014432  acc: 100.0000%(16/16)
Batch[6] - loss: 0.059493  acc: 100.0000%(16/16)
Batch[7] - loss: 0.007912  acc: 100.0000%(16/16)
Batch[8] - loss: 0.000993  acc: 100.0000%(16/16)
Batch[9] - loss: 0.055236  acc: 93.7500%(15/16)
Batch[10] - loss: 0.054888  acc: 100.0000%(16/16)
Batch[11] - loss: 0.037820  acc: 100.0000%(16/16)
Batch[12] - loss: 0.333630  acc: 93.7500%(15/16)
Batch[13] - loss: 0.010443  acc: 100.0000%(16/16)
Batch[14] - loss: 0.003752  acc: 100.0000%(16/16)
Batch[15] - loss: 0.231837  acc: 93.7500%(15/16)
Batch[16] - loss: 0.119723  acc: 93.7500%(15/16)
Batch[17] - loss: 0.005208  acc: 100.0000%(16/16)
Batch[18] - loss: 0.034085  acc: 100.0000%(16/16)
Batch[19] - loss: 0.012943  acc: 100.0000%(16/16)
Batch[20] - loss: 0.005503  acc: 100.0000%(16/16)
Batch[21] - loss: 0.007742  acc: 100.0000%(16/16)
Batch[22] - loss: 0.048564  acc: 100.0000%(16/16)
Batch[23] - loss: 0.002576  acc: 100.0000%(16/16)
Batch[24] - loss: 0.015451  acc: 100.0000%(16/16)
Batch[25] - loss: 0.003610  acc: 100.0000%(16/16)
Batch[26] - loss: 0.003010  acc: 100.0000%(16/16)
Batch[27] - loss: 0.013046  acc: 100.0000%(16/16)
Batch[28] - loss: 0.157257  acc: 93.7500%(15/16)
Batch[29] - loss: 0.001179  acc: 100.0000%(16/16)
Batch[30] - loss: 0.017170  acc: 100.0000%(16/16)
Batch[31] - loss: 0.017592  acc: 100.0000%(16/16)
Batch[32] - loss: 0.069524  acc: 100.0000%(16/16)
Batch[33] - loss: 0.010413  acc: 100.0000%(16/16)
Batch[34] - loss: 0.000776  acc: 100.0000%(16/16)
Batch[35] - loss: 0.004675  acc: 100.0000%(16/16)
Batch[36] - loss: 0.015102  acc: 100.0000%(16/16)
Batch[37] - loss: 0.002687  acc: 100.0000%(16/16)
Batch[38] - loss: 0.005634  acc: 100.0000%(16/16)
Batch[39] - loss: 0.017453  acc: 100.0000%(16/16)
Batch[40] - loss: 0.008633  acc: 100.0000%(16/16)
Batch[41] - loss: 0.034488  acc: 100.0000%(16/16)
Batch[42] - loss: 0.009292  acc: 100.0000%(16/16)
Batch[43] - loss: 0.207854  acc: 93.7500%(15/16)
Batch[44] - loss: 0.007112  acc: 100.0000%(16/16)
Batch[45] - loss: 0.006060  acc: 100.0000%(16/16)
Batch[46] - loss: 0.001884  acc: 100.0000%(16/16)
Batch[47] - loss: 0.009570  acc: 100.0000%(16/16)
Batch[48] - loss: 0.011773  acc: 100.0000%(16/16)
Batch[49] - loss: 0.012582  acc: 100.0000%(16/16)
Batch[50] - loss: 0.003906  acc: 100.0000%(16/16)
Batch[51] - loss: 0.291596  acc: 93.7500%(15/16)
Batch[52] - loss: 0.005269  acc: 100.0000%(16/16)
Batch[53] - loss: 0.006713  acc: 100.0000%(16/16)
Batch[54] - loss: 0.007664  acc: 100.0000%(16/16)
Batch[55] - loss: 0.005341  acc: 100.0000%(16/16)
Batch[56] - loss: 0.000753  acc: 100.0000%(16/16)
Batch[57] - loss: 0.060591  acc: 93.7500%(15/16)
Batch[58] - loss: 0.020453  acc: 100.0000%(16/16)
Batch[59] - loss: 0.006507  acc: 100.0000%(16/16)
Batch[60] - loss: 0.040048  acc: 100.0000%(16/16)
Batch[61] - loss: 0.003715  acc: 100.0000%(16/16)
Batch[62] - loss: 0.001632  acc: 100.0000%(13/13)
Average loss:0.038739 average acc:99.107147%
Reload the best model...
0.0005
              precision    recall  f1-score   support

          NR    0.94118   0.84211   0.88889        38
          FR    0.80556   0.78378   0.79452        37
          UR    0.68182   0.81081   0.74074        37
          TR    0.85714   0.81081   0.83333        37

    accuracy                        0.81208       149
   macro avg    0.82142   0.81188   0.81437       149
weighted avg    0.82223   0.81208   0.81487       149


Epoch  12 / 30
Batch[0] - loss: 0.032832  acc: 100.0000%(16/16)
Batch[1] - loss: 0.068235  acc: 100.0000%(16/16)
Batch[2] - loss: 0.063317  acc: 100.0000%(16/16)
Batch[3] - loss: 0.189423  acc: 93.7500%(15/16)
Batch[4] - loss: 0.084144  acc: 100.0000%(16/16)
Batch[5] - loss: 0.173793  acc: 93.7500%(15/16)
Batch[6] - loss: 0.330791  acc: 87.5000%(14/16)
Batch[7] - loss: 0.026695  acc: 100.0000%(16/16)
Batch[8] - loss: 0.015199  acc: 100.0000%(16/16)
Batch[9] - loss: 0.023761  acc: 100.0000%(16/16)
Batch[10] - loss: 0.125956  acc: 87.5000%(14/16)
Batch[11] - loss: 0.238210  acc: 93.7500%(15/16)
Batch[12] - loss: 0.081743  acc: 100.0000%(16/16)
Batch[13] - loss: 0.026003  acc: 100.0000%(16/16)
Batch[14] - loss: 0.036332  acc: 100.0000%(16/16)
Batch[15] - loss: 0.024585  acc: 100.0000%(16/16)
Batch[16] - loss: 0.018262  acc: 100.0000%(16/16)
Batch[17] - loss: 0.052613  acc: 100.0000%(16/16)
Batch[18] - loss: 0.016601  acc: 100.0000%(16/16)
Batch[19] - loss: 0.064176  acc: 100.0000%(16/16)
Batch[20] - loss: 0.005847  acc: 100.0000%(16/16)
Batch[21] - loss: 0.033107  acc: 100.0000%(16/16)
Batch[22] - loss: 0.092445  acc: 93.7500%(15/16)
Batch[23] - loss: 0.215387  acc: 87.5000%(14/16)
Batch[24] - loss: 0.102683  acc: 93.7500%(15/16)
Batch[25] - loss: 0.110057  acc: 93.7500%(15/16)
Batch[26] - loss: 0.028416  acc: 100.0000%(16/16)
Batch[27] - loss: 0.053047  acc: 93.7500%(15/16)
Batch[28] - loss: 0.014983  acc: 100.0000%(16/16)
Batch[29] - loss: 0.266026  acc: 87.5000%(14/16)
Batch[30] - loss: 0.167705  acc: 93.7500%(15/16)
Batch[31] - loss: 0.180309  acc: 93.7500%(15/16)
Batch[32] - loss: 0.101801  acc: 93.7500%(15/16)
Batch[33] - loss: 0.136627  acc: 93.7500%(15/16)
Batch[34] - loss: 0.284225  acc: 93.7500%(15/16)
Batch[35] - loss: 0.083331  acc: 93.7500%(15/16)
Batch[36] - loss: 0.042427  acc: 100.0000%(16/16)
Batch[37] - loss: 0.034657  acc: 100.0000%(16/16)
Batch[38] - loss: 0.048407  acc: 100.0000%(16/16)
Batch[39] - loss: 0.036433  acc: 100.0000%(16/16)
Batch[40] - loss: 0.027465  acc: 100.0000%(16/16)
Batch[41] - loss: 0.023919  acc: 100.0000%(16/16)
Batch[42] - loss: 0.009192  acc: 100.0000%(16/16)
Batch[43] - loss: 0.072457  acc: 93.7500%(15/16)
Batch[44] - loss: 0.301249  acc: 87.5000%(14/16)
Batch[45] - loss: 0.053481  acc: 100.0000%(16/16)
Batch[46] - loss: 0.048594  acc: 100.0000%(16/16)
Batch[47] - loss: 0.028305  acc: 100.0000%(16/16)
Batch[48] - loss: 0.072311  acc: 100.0000%(16/16)
Batch[49] - loss: 0.034064  acc: 100.0000%(16/16)
Batch[50] - loss: 0.008511  acc: 100.0000%(16/16)
Batch[51] - loss: 0.753336  acc: 93.7500%(15/16)
Batch[52] - loss: 0.089576  acc: 93.7500%(15/16)
Batch[53] - loss: 0.129877  acc: 93.7500%(15/16)
Batch[54] - loss: 0.031042  acc: 100.0000%(16/16)
Batch[55] - loss: 0.212824  acc: 93.7500%(15/16)
Batch[56] - loss: 0.020637  acc: 100.0000%(16/16)
Batch[57] - loss: 0.101224  acc: 93.7500%(15/16)
Batch[58] - loss: 0.094358  acc: 93.7500%(15/16)
Batch[59] - loss: 0.011271  acc: 100.0000%(16/16)
Batch[60] - loss: 0.017809  acc: 100.0000%(16/16)
Batch[61] - loss: 0.028067  acc: 100.0000%(16/16)
Batch[62] - loss: 0.040936  acc: 100.0000%(13/13)
Average loss:0.094303 average acc:97.023811%
              precision    recall  f1-score   support

          NR    0.89189   0.86842   0.88000        38
          FR    0.87097   0.72973   0.79412        37
          UR    0.68182   0.81081   0.74074        37
          TR    0.83784   0.83784   0.83784        37

    accuracy                        0.81208       149
   macro avg    0.82063   0.81170   0.81317       149
weighted avg    0.82111   0.81208   0.81362       149


Epoch  13 / 30
Batch[0] - loss: 0.097951  acc: 93.7500%(15/16)
Batch[1] - loss: 0.032838  acc: 100.0000%(16/16)
Batch[2] - loss: 0.037525  acc: 100.0000%(16/16)
Batch[3] - loss: 0.077435  acc: 100.0000%(16/16)
Batch[4] - loss: 0.052059  acc: 100.0000%(16/16)
Batch[5] - loss: 0.057120  acc: 100.0000%(16/16)
Batch[6] - loss: 0.010962  acc: 100.0000%(16/16)
Batch[7] - loss: 0.016417  acc: 100.0000%(16/16)
Batch[8] - loss: 0.012413  acc: 100.0000%(16/16)
Batch[9] - loss: 0.057277  acc: 100.0000%(16/16)
Batch[10] - loss: 0.036442  acc: 100.0000%(16/16)
Batch[11] - loss: 0.042101  acc: 100.0000%(16/16)
Batch[12] - loss: 0.016762  acc: 100.0000%(16/16)
Batch[13] - loss: 0.020537  acc: 100.0000%(16/16)
Batch[14] - loss: 0.061583  acc: 93.7500%(15/16)
Batch[15] - loss: 0.057486  acc: 100.0000%(16/16)
Batch[16] - loss: 0.028217  acc: 100.0000%(16/16)
Batch[17] - loss: 0.041582  acc: 100.0000%(16/16)
Batch[18] - loss: 0.067086  acc: 93.7500%(15/16)
Batch[19] - loss: 0.020561  acc: 100.0000%(16/16)
Batch[20] - loss: 0.021972  acc: 100.0000%(16/16)
Batch[21] - loss: 0.062309  acc: 100.0000%(16/16)
Batch[22] - loss: 0.331680  acc: 93.7500%(15/16)
Batch[23] - loss: 0.049697  acc: 100.0000%(16/16)
Batch[24] - loss: 0.110952  acc: 93.7500%(15/16)
Batch[25] - loss: 0.252651  acc: 93.7500%(15/16)
Batch[26] - loss: 0.042556  acc: 100.0000%(16/16)
Batch[27] - loss: 0.108218  acc: 100.0000%(16/16)
Batch[28] - loss: 0.072235  acc: 93.7500%(15/16)
Batch[29] - loss: 0.051098  acc: 100.0000%(16/16)
Batch[30] - loss: 0.068437  acc: 100.0000%(16/16)
Batch[31] - loss: 0.231342  acc: 87.5000%(14/16)
Batch[32] - loss: 0.073037  acc: 100.0000%(16/16)
Batch[33] - loss: 0.002922  acc: 100.0000%(16/16)
Batch[34] - loss: 0.058488  acc: 100.0000%(16/16)
Batch[35] - loss: 0.096717  acc: 93.7500%(15/16)
Batch[36] - loss: 0.006605  acc: 100.0000%(16/16)
Batch[37] - loss: 0.075264  acc: 100.0000%(16/16)
Batch[38] - loss: 0.017123  acc: 100.0000%(16/16)
Batch[39] - loss: 0.034744  acc: 100.0000%(16/16)
Batch[40] - loss: 0.061374  acc: 100.0000%(16/16)
Batch[41] - loss: 0.053537  acc: 100.0000%(16/16)
Batch[42] - loss: 0.026644  acc: 100.0000%(16/16)
Batch[43] - loss: 0.012205  acc: 100.0000%(16/16)
Batch[44] - loss: 0.024708  acc: 100.0000%(16/16)
Batch[45] - loss: 0.007742  acc: 100.0000%(16/16)
Batch[46] - loss: 0.160716  acc: 93.7500%(15/16)
Batch[47] - loss: 0.034205  acc: 100.0000%(16/16)
Batch[48] - loss: 0.091197  acc: 93.7500%(15/16)
Batch[49] - loss: 0.167661  acc: 93.7500%(15/16)
Batch[50] - loss: 0.012220  acc: 100.0000%(16/16)
Batch[51] - loss: 0.031534  acc: 100.0000%(16/16)
Batch[52] - loss: 0.049050  acc: 100.0000%(16/16)
Batch[53] - loss: 0.313592  acc: 93.7500%(15/16)
Batch[54] - loss: 0.010709  acc: 100.0000%(16/16)
Batch[55] - loss: 0.012708  acc: 100.0000%(16/16)
Batch[56] - loss: 0.012175  acc: 100.0000%(16/16)
Batch[57] - loss: 0.056152  acc: 100.0000%(16/16)
Batch[58] - loss: 0.051828  acc: 100.0000%(16/16)
Batch[59] - loss: 0.034715  acc: 100.0000%(16/16)
Batch[60] - loss: 0.019512  acc: 100.0000%(16/16)
Batch[61] - loss: 0.124332  acc: 93.7500%(15/16)
Batch[62] - loss: 0.004425  acc: 100.0000%(13/13)
Average loss:0.063259 average acc:98.511909%
              precision    recall  f1-score   support

          NR    0.86486   0.84211   0.85333        38
          FR    0.87097   0.72973   0.79412        37
          UR    0.72500   0.78378   0.75325        37
          TR    0.80488   0.89189   0.84615        37

    accuracy                        0.81208       149
   macro avg    0.81643   0.81188   0.81171       149
weighted avg    0.81675   0.81208   0.81199       149


Epoch  14 / 30
Batch[0] - loss: 0.015338  acc: 100.0000%(16/16)
Batch[1] - loss: 0.021174  acc: 100.0000%(16/16)
Batch[2] - loss: 0.083257  acc: 100.0000%(16/16)
Batch[3] - loss: 0.110792  acc: 93.7500%(15/16)
Batch[4] - loss: 0.024314  acc: 100.0000%(16/16)
Batch[5] - loss: 0.011606  acc: 100.0000%(16/16)
Batch[6] - loss: 0.015759  acc: 100.0000%(16/16)
Batch[7] - loss: 0.043148  acc: 100.0000%(16/16)
Batch[8] - loss: 0.001864  acc: 100.0000%(16/16)
Batch[9] - loss: 0.014222  acc: 100.0000%(16/16)
Batch[10] - loss: 0.050063  acc: 100.0000%(16/16)
Batch[11] - loss: 0.075796  acc: 93.7500%(15/16)
Batch[12] - loss: 0.010008  acc: 100.0000%(16/16)
Batch[13] - loss: 0.031263  acc: 100.0000%(16/16)
Batch[14] - loss: 0.118346  acc: 93.7500%(15/16)
Batch[15] - loss: 0.074254  acc: 93.7500%(15/16)
Batch[16] - loss: 0.038813  acc: 100.0000%(16/16)
Batch[17] - loss: 0.076234  acc: 100.0000%(16/16)
Batch[18] - loss: 0.008651  acc: 100.0000%(16/16)
Batch[19] - loss: 0.193253  acc: 93.7500%(15/16)
Batch[20] - loss: 0.026054  acc: 100.0000%(16/16)
Batch[21] - loss: 0.016096  acc: 100.0000%(16/16)
Batch[22] - loss: 0.035865  acc: 100.0000%(16/16)
Batch[23] - loss: 0.011519  acc: 100.0000%(16/16)
Batch[24] - loss: 0.067285  acc: 93.7500%(15/16)
Batch[25] - loss: 0.011181  acc: 100.0000%(16/16)
Batch[26] - loss: 0.050112  acc: 93.7500%(15/16)
Batch[27] - loss: 0.049237  acc: 100.0000%(16/16)
Batch[28] - loss: 0.021147  acc: 100.0000%(16/16)
Batch[29] - loss: 0.006335  acc: 100.0000%(16/16)
Batch[30] - loss: 0.105525  acc: 93.7500%(15/16)
Batch[31] - loss: 0.008032  acc: 100.0000%(16/16)
Batch[32] - loss: 0.041066  acc: 100.0000%(16/16)
Batch[33] - loss: 0.024147  acc: 100.0000%(16/16)
Batch[34] - loss: 0.006414  acc: 100.0000%(16/16)
Batch[35] - loss: 0.120632  acc: 93.7500%(15/16)
Batch[36] - loss: 0.155467  acc: 93.7500%(15/16)
Batch[37] - loss: 0.005009  acc: 100.0000%(16/16)
Batch[38] - loss: 0.032400  acc: 100.0000%(16/16)
Batch[39] - loss: 0.033269  acc: 100.0000%(16/16)
Batch[40] - loss: 0.010968  acc: 100.0000%(16/16)
Batch[41] - loss: 0.047696  acc: 100.0000%(16/16)
Batch[42] - loss: 0.125605  acc: 93.7500%(15/16)
Batch[43] - loss: 0.010402  acc: 100.0000%(16/16)
Batch[44] - loss: 0.050232  acc: 100.0000%(16/16)
Batch[45] - loss: 0.050408  acc: 100.0000%(16/16)
Batch[46] - loss: 0.002727  acc: 100.0000%(16/16)
Batch[47] - loss: 0.094601  acc: 93.7500%(15/16)
Batch[48] - loss: 0.013648  acc: 100.0000%(16/16)
Batch[49] - loss: 0.004943  acc: 100.0000%(16/16)
Batch[50] - loss: 0.023813  acc: 100.0000%(16/16)
Batch[51] - loss: 0.008790  acc: 100.0000%(16/16)
Batch[52] - loss: 0.012884  acc: 100.0000%(16/16)
Batch[53] - loss: 0.074532  acc: 100.0000%(16/16)
Batch[54] - loss: 0.010964  acc: 100.0000%(16/16)
Batch[55] - loss: 0.068551  acc: 100.0000%(16/16)
Batch[56] - loss: 0.012442  acc: 100.0000%(16/16)
Batch[57] - loss: 0.041733  acc: 100.0000%(16/16)
Batch[58] - loss: 0.003293  acc: 100.0000%(16/16)
Batch[59] - loss: 0.045935  acc: 100.0000%(16/16)
Batch[60] - loss: 0.016159  acc: 100.0000%(16/16)
Batch[61] - loss: 0.020679  acc: 100.0000%(16/16)
Batch[62] - loss: 0.002741  acc: 100.0000%(13/13)
Average loss:0.041249 average acc:98.809532%
              precision    recall  f1-score   support

          NR    0.82051   0.84211   0.83117        38
          FR    0.93103   0.72973   0.81818        37
          UR    0.65217   0.81081   0.72289        37
          TR    0.88571   0.83784   0.86111        37

    accuracy                        0.80537       149
   macro avg    0.82236   0.80512   0.80834       149
weighted avg    0.82235   0.80537   0.80849       149


Epoch  15 / 30
Batch[0] - loss: 0.010360  acc: 100.0000%(16/16)
Batch[1] - loss: 0.194544  acc: 93.7500%(15/16)
Batch[2] - loss: 0.092929  acc: 93.7500%(15/16)
Batch[3] - loss: 0.040816  acc: 100.0000%(16/16)
Batch[4] - loss: 0.001542  acc: 100.0000%(16/16)
Batch[5] - loss: 0.036308  acc: 100.0000%(16/16)
Batch[6] - loss: 0.005590  acc: 100.0000%(16/16)
Batch[7] - loss: 0.049795  acc: 100.0000%(16/16)
Batch[8] - loss: 0.004468  acc: 100.0000%(16/16)
Batch[9] - loss: 0.014526  acc: 100.0000%(16/16)
Batch[10] - loss: 0.009094  acc: 100.0000%(16/16)
Batch[11] - loss: 0.141441  acc: 93.7500%(15/16)
Batch[12] - loss: 0.024417  acc: 100.0000%(16/16)
Batch[13] - loss: 0.040382  acc: 100.0000%(16/16)
Batch[14] - loss: 0.010508  acc: 100.0000%(16/16)
Batch[15] - loss: 0.046289  acc: 100.0000%(16/16)
Batch[16] - loss: 0.042471  acc: 100.0000%(16/16)
Batch[17] - loss: 0.068135  acc: 100.0000%(16/16)
Batch[18] - loss: 0.414145  acc: 93.7500%(15/16)
Batch[19] - loss: 0.029025  acc: 100.0000%(16/16)
Batch[20] - loss: 0.006379  acc: 100.0000%(16/16)
Batch[21] - loss: 0.001952  acc: 100.0000%(16/16)
Batch[22] - loss: 0.013538  acc: 100.0000%(16/16)
Batch[23] - loss: 0.024974  acc: 100.0000%(16/16)
Batch[24] - loss: 0.059295  acc: 100.0000%(16/16)
Batch[25] - loss: 0.007818  acc: 100.0000%(16/16)
Batch[26] - loss: 0.094806  acc: 100.0000%(16/16)
Batch[27] - loss: 0.003327  acc: 100.0000%(16/16)
Batch[28] - loss: 0.002867  acc: 100.0000%(16/16)
Batch[29] - loss: 0.031173  acc: 100.0000%(16/16)
Batch[30] - loss: 0.002563  acc: 100.0000%(16/16)
Batch[31] - loss: 0.022698  acc: 100.0000%(16/16)
Batch[32] - loss: 0.012036  acc: 100.0000%(16/16)
Batch[33] - loss: 0.091817  acc: 93.7500%(15/16)
Batch[34] - loss: 0.006176  acc: 100.0000%(16/16)
Batch[35] - loss: 0.026839  acc: 100.0000%(16/16)
Batch[36] - loss: 0.010560  acc: 100.0000%(16/16)
Batch[37] - loss: 0.028926  acc: 100.0000%(16/16)
Batch[38] - loss: 0.010459  acc: 100.0000%(16/16)
Batch[39] - loss: 0.014516  acc: 100.0000%(16/16)
Batch[40] - loss: 0.007121  acc: 100.0000%(16/16)
Batch[41] - loss: 0.007588  acc: 100.0000%(16/16)
Batch[42] - loss: 0.011319  acc: 100.0000%(16/16)
Batch[43] - loss: 0.171459  acc: 93.7500%(15/16)
Batch[44] - loss: 0.038028  acc: 100.0000%(16/16)
Batch[45] - loss: 0.021888  acc: 100.0000%(16/16)
Batch[46] - loss: 0.257751  acc: 87.5000%(14/16)
Batch[47] - loss: 0.027153  acc: 100.0000%(16/16)
Batch[48] - loss: 0.130417  acc: 93.7500%(15/16)
Batch[49] - loss: 0.343454  acc: 93.7500%(15/16)
Batch[50] - loss: 0.065950  acc: 93.7500%(15/16)
Batch[51] - loss: 0.072896  acc: 93.7500%(15/16)
Batch[52] - loss: 0.005946  acc: 100.0000%(16/16)
Batch[53] - loss: 0.008663  acc: 100.0000%(16/16)
Batch[54] - loss: 0.037340  acc: 100.0000%(16/16)
Batch[55] - loss: 0.016595  acc: 100.0000%(16/16)
Batch[56] - loss: 0.045874  acc: 100.0000%(16/16)
Batch[57] - loss: 0.012056  acc: 100.0000%(16/16)
Batch[58] - loss: 0.019673  acc: 100.0000%(16/16)
Batch[59] - loss: 0.158214  acc: 93.7500%(15/16)
Batch[60] - loss: 0.000973  acc: 100.0000%(16/16)
Batch[61] - loss: 0.038645  acc: 100.0000%(16/16)
Batch[62] - loss: 0.124431  acc: 92.3077%(12/13)
Average loss:0.053539 average acc:98.588219%
Reload the best model...
0.00025
              precision    recall  f1-score   support

          NR    0.94118   0.84211   0.88889        38
          FR    0.80556   0.78378   0.79452        37
          UR    0.68182   0.81081   0.74074        37
          TR    0.85714   0.81081   0.83333        37

    accuracy                        0.81208       149
   macro avg    0.82142   0.81188   0.81437       149
weighted avg    0.82223   0.81208   0.81487       149


Epoch  16 / 30
Batch[0] - loss: 0.145422  acc: 93.7500%(15/16)
Batch[1] - loss: 0.221645  acc: 93.7500%(15/16)
Batch[2] - loss: 0.026398  acc: 100.0000%(16/16)
Batch[3] - loss: 0.107361  acc: 93.7500%(15/16)
Batch[4] - loss: 0.312945  acc: 93.7500%(15/16)
Batch[5] - loss: 0.019942  acc: 100.0000%(16/16)
Batch[6] - loss: 0.097254  acc: 93.7500%(15/16)
Batch[7] - loss: 0.096595  acc: 93.7500%(15/16)
Batch[8] - loss: 0.115897  acc: 93.7500%(15/16)
Batch[9] - loss: 0.008215  acc: 100.0000%(16/16)
Batch[10] - loss: 0.045584  acc: 100.0000%(16/16)
Batch[11] - loss: 0.112452  acc: 93.7500%(15/16)
Batch[12] - loss: 0.006707  acc: 100.0000%(16/16)
Batch[13] - loss: 0.053822  acc: 100.0000%(16/16)
Batch[14] - loss: 0.055921  acc: 100.0000%(16/16)
Batch[15] - loss: 0.087547  acc: 93.7500%(15/16)
Batch[16] - loss: 0.060632  acc: 100.0000%(16/16)
Batch[17] - loss: 0.183978  acc: 87.5000%(14/16)
Batch[18] - loss: 0.241234  acc: 87.5000%(14/16)
Batch[19] - loss: 0.018878  acc: 100.0000%(16/16)
Batch[20] - loss: 0.121170  acc: 93.7500%(15/16)
Batch[21] - loss: 0.294074  acc: 93.7500%(15/16)
Batch[22] - loss: 0.096088  acc: 100.0000%(16/16)
Batch[23] - loss: 0.180325  acc: 93.7500%(15/16)
Batch[24] - loss: 0.047447  acc: 100.0000%(16/16)
Batch[25] - loss: 0.026117  acc: 100.0000%(16/16)
Batch[26] - loss: 0.113580  acc: 93.7500%(15/16)
Batch[27] - loss: 0.094159  acc: 93.7500%(15/16)
Batch[28] - loss: 0.019076  acc: 100.0000%(16/16)
Batch[29] - loss: 0.014310  acc: 100.0000%(16/16)
Batch[30] - loss: 0.013187  acc: 100.0000%(16/16)
Batch[31] - loss: 0.339069  acc: 93.7500%(15/16)
Batch[32] - loss: 0.093394  acc: 100.0000%(16/16)
Batch[33] - loss: 0.378151  acc: 87.5000%(14/16)
Batch[34] - loss: 0.093910  acc: 93.7500%(15/16)
Batch[35] - loss: 0.044541  acc: 100.0000%(16/16)
Batch[36] - loss: 0.125229  acc: 93.7500%(15/16)
Batch[37] - loss: 0.101489  acc: 93.7500%(15/16)
Batch[38] - loss: 0.008298  acc: 100.0000%(16/16)
Batch[39] - loss: 0.006365  acc: 100.0000%(16/16)
Batch[40] - loss: 0.061944  acc: 100.0000%(16/16)
Batch[41] - loss: 0.025106  acc: 100.0000%(16/16)
Batch[42] - loss: 0.024472  acc: 100.0000%(16/16)
Batch[43] - loss: 0.038711  acc: 100.0000%(16/16)
Batch[44] - loss: 0.036653  acc: 100.0000%(16/16)
Batch[45] - loss: 0.021531  acc: 100.0000%(16/16)
Batch[46] - loss: 0.091933  acc: 100.0000%(16/16)
Batch[47] - loss: 0.078184  acc: 93.7500%(15/16)
Batch[48] - loss: 0.038237  acc: 100.0000%(16/16)
Batch[49] - loss: 0.005685  acc: 100.0000%(16/16)
Batch[50] - loss: 0.053905  acc: 100.0000%(16/16)
Batch[51] - loss: 0.058383  acc: 93.7500%(15/16)
Batch[52] - loss: 0.016485  acc: 100.0000%(16/16)
Batch[53] - loss: 0.008731  acc: 100.0000%(16/16)
Batch[54] - loss: 0.115531  acc: 93.7500%(15/16)
Batch[55] - loss: 0.219243  acc: 87.5000%(14/16)
Batch[56] - loss: 0.066285  acc: 93.7500%(15/16)
Batch[57] - loss: 0.059252  acc: 100.0000%(16/16)
Batch[58] - loss: 0.093024  acc: 93.7500%(15/16)
Batch[59] - loss: 0.145010  acc: 100.0000%(16/16)
Batch[60] - loss: 0.164538  acc: 87.5000%(14/16)
Batch[61] - loss: 0.080238  acc: 93.7500%(15/16)
Batch[62] - loss: 0.005994  acc: 100.0000%(13/13)
Average loss:0.091071 average acc:96.626991%
              precision    recall  f1-score   support

          NR    0.96667   0.76316   0.85294        38
          FR    0.90000   0.72973   0.80597        37
          UR    0.58929   0.89189   0.70968        37
          TR    0.90909   0.81081   0.85714        37

    accuracy                        0.79866       149
   macro avg    0.84126   0.79890   0.80643       149
weighted avg    0.84210   0.79866   0.80675       149


Epoch  17 / 30
Batch[0] - loss: 0.013617  acc: 100.0000%(16/16)
Batch[1] - loss: 0.122168  acc: 93.7500%(15/16)
Batch[2] - loss: 0.013041  acc: 100.0000%(16/16)
Batch[3] - loss: 0.020716  acc: 100.0000%(16/16)
Batch[4] - loss: 0.033890  acc: 100.0000%(16/16)
Batch[5] - loss: 0.005513  acc: 100.0000%(16/16)
Batch[6] - loss: 0.141750  acc: 93.7500%(15/16)
Batch[7] - loss: 0.078458  acc: 100.0000%(16/16)
Batch[8] - loss: 0.007971  acc: 100.0000%(16/16)
Batch[9] - loss: 0.005914  acc: 100.0000%(16/16)
Batch[10] - loss: 0.077464  acc: 93.7500%(15/16)
Batch[11] - loss: 0.039264  acc: 100.0000%(16/16)
Batch[12] - loss: 0.026824  acc: 100.0000%(16/16)
Batch[13] - loss: 0.011308  acc: 100.0000%(16/16)
Batch[14] - loss: 0.252688  acc: 93.7500%(15/16)
Batch[15] - loss: 0.043273  acc: 100.0000%(16/16)
Batch[16] - loss: 0.019492  acc: 100.0000%(16/16)
Batch[17] - loss: 0.071437  acc: 100.0000%(16/16)
Batch[18] - loss: 0.167861  acc: 93.7500%(15/16)
Batch[19] - loss: 0.096874  acc: 93.7500%(15/16)
Batch[20] - loss: 0.097863  acc: 93.7500%(15/16)
Batch[21] - loss: 0.062157  acc: 100.0000%(16/16)
Batch[22] - loss: 0.056201  acc: 100.0000%(16/16)
Batch[23] - loss: 0.177783  acc: 93.7500%(15/16)
Batch[24] - loss: 0.017253  acc: 100.0000%(16/16)
Batch[25] - loss: 0.014154  acc: 100.0000%(16/16)
Batch[26] - loss: 0.024814  acc: 100.0000%(16/16)
Batch[27] - loss: 0.051957  acc: 100.0000%(16/16)
Batch[28] - loss: 0.021514  acc: 100.0000%(16/16)
Batch[29] - loss: 0.067197  acc: 100.0000%(16/16)
Batch[30] - loss: 0.014743  acc: 100.0000%(16/16)
Batch[31] - loss: 0.039740  acc: 100.0000%(16/16)
Batch[32] - loss: 0.005364  acc: 100.0000%(16/16)
Batch[33] - loss: 0.010407  acc: 100.0000%(16/16)
Batch[34] - loss: 0.003309  acc: 100.0000%(16/16)
Batch[35] - loss: 0.010622  acc: 100.0000%(16/16)
Batch[36] - loss: 0.151514  acc: 93.7500%(15/16)
Batch[37] - loss: 0.016707  acc: 100.0000%(16/16)
Batch[38] - loss: 0.011703  acc: 100.0000%(16/16)
Batch[39] - loss: 0.025538  acc: 100.0000%(16/16)
Batch[40] - loss: 0.115227  acc: 93.7500%(15/16)
Batch[41] - loss: 0.066828  acc: 100.0000%(16/16)
Batch[42] - loss: 0.006552  acc: 100.0000%(16/16)
Batch[43] - loss: 0.008101  acc: 100.0000%(16/16)
Batch[44] - loss: 0.042769  acc: 100.0000%(16/16)
Batch[45] - loss: 0.051153  acc: 100.0000%(16/16)
Batch[46] - loss: 0.021234  acc: 100.0000%(16/16)
Batch[47] - loss: 0.042268  acc: 100.0000%(16/16)
Batch[48] - loss: 0.008860  acc: 100.0000%(16/16)
Batch[49] - loss: 0.154520  acc: 93.7500%(15/16)
Batch[50] - loss: 0.077475  acc: 93.7500%(15/16)
Batch[51] - loss: 0.139166  acc: 93.7500%(15/16)
Batch[52] - loss: 0.346949  acc: 81.2500%(13/16)
Batch[53] - loss: 0.108718  acc: 93.7500%(15/16)
Batch[54] - loss: 0.020433  acc: 100.0000%(16/16)
Batch[55] - loss: 0.021772  acc: 100.0000%(16/16)
Batch[56] - loss: 0.084269  acc: 93.7500%(15/16)
Batch[57] - loss: 0.128793  acc: 93.7500%(15/16)
Batch[58] - loss: 0.085464  acc: 100.0000%(16/16)
Batch[59] - loss: 0.023551  acc: 100.0000%(16/16)
Batch[60] - loss: 0.052920  acc: 100.0000%(16/16)
Batch[61] - loss: 0.016958  acc: 100.0000%(16/16)
Batch[62] - loss: 0.016335  acc: 100.0000%(13/13)
Average loss:0.059847 average acc:98.115082%
              precision    recall  f1-score   support

          NR    0.96875   0.81579   0.88571        38
          FR    0.89655   0.70270   0.78788        37
          UR    0.59259   0.86486   0.70330        37
          TR    0.88235   0.81081   0.84507        37

    accuracy                        0.79866       149
   macro avg    0.83506   0.79854   0.80549       149
weighted avg    0.83596   0.79866   0.80603       149


Epoch  18 / 30
Batch[0] - loss: 0.061411  acc: 100.0000%(16/16)
Batch[1] - loss: 0.052405  acc: 100.0000%(16/16)
Batch[2] - loss: 0.011906  acc: 100.0000%(16/16)
Batch[3] - loss: 0.068998  acc: 100.0000%(16/16)
Batch[4] - loss: 0.453322  acc: 93.7500%(15/16)
Batch[5] - loss: 0.007812  acc: 100.0000%(16/16)
Batch[6] - loss: 0.070301  acc: 100.0000%(16/16)
Batch[7] - loss: 0.005007  acc: 100.0000%(16/16)
Batch[8] - loss: 0.033844  acc: 100.0000%(16/16)
Batch[9] - loss: 0.027492  acc: 100.0000%(16/16)
Batch[10] - loss: 0.006322  acc: 100.0000%(16/16)
Batch[11] - loss: 0.059554  acc: 100.0000%(16/16)
Batch[12] - loss: 0.029105  acc: 100.0000%(16/16)
Batch[13] - loss: 0.058375  acc: 100.0000%(16/16)
Batch[14] - loss: 0.012552  acc: 100.0000%(16/16)
Batch[15] - loss: 0.085299  acc: 93.7500%(15/16)
Batch[16] - loss: 0.033187  acc: 100.0000%(16/16)
Batch[17] - loss: 0.025112  acc: 100.0000%(16/16)
Batch[18] - loss: 0.005976  acc: 100.0000%(16/16)
Batch[19] - loss: 0.034495  acc: 100.0000%(16/16)
Batch[20] - loss: 0.244278  acc: 93.7500%(15/16)
Batch[21] - loss: 0.004718  acc: 100.0000%(16/16)
Batch[22] - loss: 0.061867  acc: 100.0000%(16/16)
Batch[23] - loss: 0.080241  acc: 100.0000%(16/16)
Batch[24] - loss: 0.045162  acc: 100.0000%(16/16)
Batch[25] - loss: 0.372456  acc: 87.5000%(14/16)
Batch[26] - loss: 0.020494  acc: 100.0000%(16/16)
Batch[27] - loss: 0.002691  acc: 100.0000%(16/16)
Batch[28] - loss: 0.115725  acc: 100.0000%(16/16)
Batch[29] - loss: 0.189551  acc: 93.7500%(15/16)
Batch[30] - loss: 0.007476  acc: 100.0000%(16/16)
Batch[31] - loss: 0.041700  acc: 100.0000%(16/16)
Batch[32] - loss: 0.018967  acc: 100.0000%(16/16)
Batch[33] - loss: 0.107922  acc: 93.7500%(15/16)
Batch[34] - loss: 0.620096  acc: 87.5000%(14/16)
Batch[35] - loss: 0.020347  acc: 100.0000%(16/16)
Batch[36] - loss: 0.009427  acc: 100.0000%(16/16)
Batch[37] - loss: 0.030835  acc: 100.0000%(16/16)
Batch[38] - loss: 0.020204  acc: 100.0000%(16/16)
Batch[39] - loss: 0.007531  acc: 100.0000%(16/16)
Batch[40] - loss: 0.032828  acc: 100.0000%(16/16)
Batch[41] - loss: 0.030518  acc: 100.0000%(16/16)
Batch[42] - loss: 0.077912  acc: 100.0000%(16/16)
Batch[43] - loss: 0.074126  acc: 93.7500%(15/16)
Batch[44] - loss: 0.064317  acc: 100.0000%(16/16)
Batch[45] - loss: 0.039750  acc: 100.0000%(16/16)
Batch[46] - loss: 0.202715  acc: 93.7500%(15/16)
Batch[47] - loss: 0.046139  acc: 100.0000%(16/16)
Batch[48] - loss: 0.019143  acc: 100.0000%(16/16)
Batch[49] - loss: 0.017579  acc: 100.0000%(16/16)
Batch[50] - loss: 0.244450  acc: 93.7500%(15/16)
Batch[51] - loss: 0.165732  acc: 87.5000%(14/16)
Batch[52] - loss: 0.419638  acc: 93.7500%(15/16)
Batch[53] - loss: 0.049015  acc: 100.0000%(16/16)
Batch[54] - loss: 0.011790  acc: 100.0000%(16/16)
Batch[55] - loss: 0.062762  acc: 100.0000%(16/16)
Batch[56] - loss: 0.023112  acc: 100.0000%(16/16)
Batch[57] - loss: 0.097352  acc: 93.7500%(15/16)
Batch[58] - loss: 0.031466  acc: 100.0000%(16/16)
Batch[59] - loss: 0.074503  acc: 100.0000%(16/16)
Batch[60] - loss: 0.003760  acc: 100.0000%(16/16)
Batch[61] - loss: 0.211546  acc: 93.7500%(15/16)
Batch[62] - loss: 0.101901  acc: 92.3077%(12/13)
Average loss:0.083591 average acc:98.191399%
              precision    recall  f1-score   support

          NR    0.86111   0.81579   0.83784        38
          FR    0.92593   0.67568   0.78125        37
          UR    0.59615   0.83784   0.69663        37
          TR    0.88235   0.81081   0.84507        37

    accuracy                        0.78523       149
   macro avg    0.81639   0.78503   0.79020       149
weighted avg    0.81669   0.78523   0.79052       149


Epoch  19 / 30
Batch[0] - loss: 0.126286  acc: 93.7500%(15/16)
Batch[1] - loss: 0.128578  acc: 93.7500%(15/16)
Batch[2] - loss: 0.045010  acc: 100.0000%(16/16)
Batch[3] - loss: 0.163178  acc: 93.7500%(15/16)
Batch[4] - loss: 0.012457  acc: 100.0000%(16/16)
Batch[5] - loss: 0.003848  acc: 100.0000%(16/16)
Batch[6] - loss: 0.067888  acc: 100.0000%(16/16)
Batch[7] - loss: 0.005328  acc: 100.0000%(16/16)
Batch[8] - loss: 0.009163  acc: 100.0000%(16/16)
Batch[9] - loss: 0.124447  acc: 93.7500%(15/16)
Batch[10] - loss: 0.009257  acc: 100.0000%(16/16)
Batch[11] - loss: 0.005932  acc: 100.0000%(16/16)
Batch[12] - loss: 0.025885  acc: 100.0000%(16/16)
Batch[13] - loss: 0.073459  acc: 93.7500%(15/16)
Batch[14] - loss: 0.151371  acc: 93.7500%(15/16)
Batch[15] - loss: 0.018672  acc: 100.0000%(16/16)
Batch[16] - loss: 0.025822  acc: 100.0000%(16/16)
Batch[17] - loss: 0.009485  acc: 100.0000%(16/16)
Batch[18] - loss: 0.008634  acc: 100.0000%(16/16)
Batch[19] - loss: 0.016440  acc: 100.0000%(16/16)
Batch[20] - loss: 0.015789  acc: 100.0000%(16/16)
Batch[21] - loss: 0.052459  acc: 100.0000%(16/16)
Batch[22] - loss: 0.043042  acc: 100.0000%(16/16)
Batch[23] - loss: 0.012590  acc: 100.0000%(16/16)
Batch[24] - loss: 0.006661  acc: 100.0000%(16/16)
Batch[25] - loss: 0.059309  acc: 93.7500%(15/16)
Batch[26] - loss: 0.171215  acc: 87.5000%(14/16)
Batch[27] - loss: 0.062298  acc: 100.0000%(16/16)
Batch[28] - loss: 0.059521  acc: 100.0000%(16/16)
Batch[29] - loss: 0.018634  acc: 100.0000%(16/16)
Batch[30] - loss: 0.257128  acc: 93.7500%(15/16)
Batch[31] - loss: 0.089996  acc: 93.7500%(15/16)
Batch[32] - loss: 0.190508  acc: 93.7500%(15/16)
Batch[33] - loss: 0.064948  acc: 100.0000%(16/16)
Batch[34] - loss: 0.010443  acc: 100.0000%(16/16)
Batch[35] - loss: 0.049998  acc: 100.0000%(16/16)
Batch[36] - loss: 0.008436  acc: 100.0000%(16/16)
Batch[37] - loss: 0.109312  acc: 93.7500%(15/16)
Batch[38] - loss: 0.009733  acc: 100.0000%(16/16)
Batch[39] - loss: 0.016845  acc: 100.0000%(16/16)
Batch[40] - loss: 0.037026  acc: 100.0000%(16/16)
Batch[41] - loss: 0.031776  acc: 100.0000%(16/16)
Batch[42] - loss: 0.029339  acc: 100.0000%(16/16)
Batch[43] - loss: 0.294325  acc: 93.7500%(15/16)
Batch[44] - loss: 0.025681  acc: 100.0000%(16/16)
Batch[45] - loss: 0.026520  acc: 100.0000%(16/16)
Batch[46] - loss: 0.004289  acc: 100.0000%(16/16)
Batch[47] - loss: 0.011718  acc: 100.0000%(16/16)
Batch[48] - loss: 0.009000  acc: 100.0000%(16/16)
Batch[49] - loss: 0.006359  acc: 100.0000%(16/16)
Batch[50] - loss: 0.030573  acc: 100.0000%(16/16)
Batch[51] - loss: 0.004623  acc: 100.0000%(16/16)
Batch[52] - loss: 0.293214  acc: 93.7500%(15/16)
Batch[53] - loss: 0.011124  acc: 100.0000%(16/16)
Batch[54] - loss: 0.060685  acc: 100.0000%(16/16)
Batch[55] - loss: 0.021570  acc: 100.0000%(16/16)
Batch[56] - loss: 0.005384  acc: 100.0000%(16/16)
Batch[57] - loss: 0.047470  acc: 100.0000%(16/16)
Batch[58] - loss: 0.059697  acc: 100.0000%(16/16)
Batch[59] - loss: 0.014343  acc: 100.0000%(16/16)
Batch[60] - loss: 0.039680  acc: 100.0000%(16/16)
Batch[61] - loss: 0.016482  acc: 100.0000%(16/16)
Batch[62] - loss: 0.081215  acc: 100.0000%(13/13)
Average loss:0.055589 average acc:98.511909%
Reload the best model...
0.000125
              precision    recall  f1-score   support

          NR    0.94118   0.84211   0.88889        38
          FR    0.80556   0.78378   0.79452        37
          UR    0.68182   0.81081   0.74074        37
          TR    0.85714   0.81081   0.83333        37

    accuracy                        0.81208       149
   macro avg    0.82142   0.81188   0.81437       149
weighted avg    0.82223   0.81208   0.81487       149


Epoch  20 / 30
Batch[0] - loss: 0.031004  acc: 100.0000%(16/16)
Batch[1] - loss: 0.132112  acc: 93.7500%(15/16)
Batch[2] - loss: 0.011343  acc: 100.0000%(16/16)
Batch[3] - loss: 0.027687  acc: 100.0000%(16/16)
Batch[4] - loss: 0.028940  acc: 100.0000%(16/16)
Batch[5] - loss: 0.134941  acc: 93.7500%(15/16)
Batch[6] - loss: 0.096451  acc: 100.0000%(16/16)
Batch[7] - loss: 0.053264  acc: 100.0000%(16/16)
Batch[8] - loss: 0.057900  acc: 100.0000%(16/16)
Batch[9] - loss: 0.007779  acc: 100.0000%(16/16)
Batch[10] - loss: 0.035396  acc: 100.0000%(16/16)
Batch[11] - loss: 0.024081  acc: 100.0000%(16/16)
Batch[12] - loss: 0.099512  acc: 93.7500%(15/16)
Batch[13] - loss: 0.131996  acc: 93.7500%(15/16)
Batch[14] - loss: 0.024228  acc: 100.0000%(16/16)
Batch[15] - loss: 0.304915  acc: 93.7500%(15/16)
Batch[16] - loss: 0.170900  acc: 87.5000%(14/16)
Batch[17] - loss: 0.021033  acc: 100.0000%(16/16)
Batch[18] - loss: 0.268301  acc: 93.7500%(15/16)
Batch[19] - loss: 0.044821  acc: 100.0000%(16/16)
Batch[20] - loss: 0.040610  acc: 100.0000%(16/16)
Batch[21] - loss: 0.105897  acc: 93.7500%(15/16)
Batch[22] - loss: 0.347002  acc: 93.7500%(15/16)
Batch[23] - loss: 0.011159  acc: 100.0000%(16/16)
Batch[24] - loss: 0.066620  acc: 100.0000%(16/16)
Batch[25] - loss: 0.047025  acc: 100.0000%(16/16)
Batch[26] - loss: 0.310330  acc: 87.5000%(14/16)
Batch[27] - loss: 0.155478  acc: 93.7500%(15/16)
Batch[28] - loss: 0.096539  acc: 93.7500%(15/16)
Batch[29] - loss: 0.100513  acc: 100.0000%(16/16)
Batch[30] - loss: 0.020958  acc: 100.0000%(16/16)
Batch[31] - loss: 0.021411  acc: 100.0000%(16/16)
Batch[32] - loss: 0.047935  acc: 100.0000%(16/16)
Batch[33] - loss: 0.035060  acc: 100.0000%(16/16)
Batch[34] - loss: 0.053449  acc: 100.0000%(16/16)
Batch[35] - loss: 0.034772  acc: 100.0000%(16/16)
Batch[36] - loss: 0.025499  acc: 100.0000%(16/16)
Batch[37] - loss: 0.035786  acc: 100.0000%(16/16)
Batch[38] - loss: 0.112657  acc: 93.7500%(15/16)
Batch[39] - loss: 0.055029  acc: 100.0000%(16/16)
Batch[40] - loss: 0.019663  acc: 100.0000%(16/16)
Batch[41] - loss: 0.140413  acc: 93.7500%(15/16)
Batch[42] - loss: 0.284516  acc: 87.5000%(14/16)
Batch[43] - loss: 0.005099  acc: 100.0000%(16/16)
Batch[44] - loss: 0.122813  acc: 100.0000%(16/16)
Batch[45] - loss: 0.097369  acc: 93.7500%(15/16)
Batch[46] - loss: 0.014473  acc: 100.0000%(16/16)
Batch[47] - loss: 0.136976  acc: 93.7500%(15/16)
Batch[48] - loss: 0.027412  acc: 100.0000%(16/16)
Batch[49] - loss: 0.052901  acc: 100.0000%(16/16)
Batch[50] - loss: 0.017628  acc: 100.0000%(16/16)
Batch[51] - loss: 0.036173  acc: 100.0000%(16/16)
Batch[52] - loss: 0.008230  acc: 100.0000%(16/16)
Batch[53] - loss: 0.030436  acc: 100.0000%(16/16)
Batch[54] - loss: 0.015730  acc: 100.0000%(16/16)
Batch[55] - loss: 0.083463  acc: 93.7500%(15/16)
Batch[56] - loss: 0.022323  acc: 100.0000%(16/16)
Batch[57] - loss: 0.042862  acc: 100.0000%(16/16)
Batch[58] - loss: 0.057588  acc: 100.0000%(16/16)
Batch[59] - loss: 0.010934  acc: 100.0000%(16/16)
Batch[60] - loss: 0.009374  acc: 100.0000%(16/16)
Batch[61] - loss: 0.089508  acc: 93.7500%(15/16)
Batch[62] - loss: 0.114476  acc: 92.3077%(12/13)
Average loss:0.077313 average acc:97.695366%
              precision    recall  f1-score   support

          NR    0.91176   0.81579   0.86111        38
          FR    0.86667   0.70270   0.77612        37
          UR    0.62745   0.86486   0.72727        37
          TR    0.88235   0.81081   0.84507        37

    accuracy                        0.79866       149
   macro avg    0.82206   0.79854   0.80239       149
weighted avg    0.82266   0.79866   0.80279       149


Epoch  21 / 30
Batch[0] - loss: 0.064740  acc: 93.7500%(15/16)
Batch[1] - loss: 0.025233  acc: 100.0000%(16/16)
Batch[2] - loss: 0.011965  acc: 100.0000%(16/16)
Batch[3] - loss: 0.115152  acc: 93.7500%(15/16)
Batch[4] - loss: 0.068097  acc: 100.0000%(16/16)
Batch[5] - loss: 0.059199  acc: 100.0000%(16/16)
Batch[6] - loss: 0.077155  acc: 100.0000%(16/16)
Batch[7] - loss: 0.143475  acc: 93.7500%(15/16)
Batch[8] - loss: 0.013686  acc: 100.0000%(16/16)
Batch[9] - loss: 0.040991  acc: 100.0000%(16/16)
Batch[10] - loss: 0.035027  acc: 100.0000%(16/16)
Batch[11] - loss: 0.186216  acc: 87.5000%(14/16)
Batch[12] - loss: 0.026377  acc: 100.0000%(16/16)
Batch[13] - loss: 0.101569  acc: 93.7500%(15/16)
Batch[14] - loss: 0.070118  acc: 93.7500%(15/16)
Batch[15] - loss: 0.040350  acc: 100.0000%(16/16)
Batch[16] - loss: 0.071373  acc: 100.0000%(16/16)
Batch[17] - loss: 0.166541  acc: 93.7500%(15/16)
Batch[18] - loss: 0.592937  acc: 87.5000%(14/16)
Batch[19] - loss: 0.081664  acc: 93.7500%(15/16)
Batch[20] - loss: 0.052333  acc: 93.7500%(15/16)
Batch[21] - loss: 0.161271  acc: 87.5000%(14/16)
Batch[22] - loss: 0.020596  acc: 100.0000%(16/16)
Batch[23] - loss: 0.013589  acc: 100.0000%(16/16)
Batch[24] - loss: 0.079684  acc: 100.0000%(16/16)
Batch[25] - loss: 0.079347  acc: 93.7500%(15/16)
Batch[26] - loss: 0.022771  acc: 100.0000%(16/16)
Batch[27] - loss: 0.010330  acc: 100.0000%(16/16)
Batch[28] - loss: 0.045730  acc: 100.0000%(16/16)
Batch[29] - loss: 0.042714  acc: 100.0000%(16/16)
Batch[30] - loss: 0.066960  acc: 100.0000%(16/16)
Batch[31] - loss: 0.002616  acc: 100.0000%(16/16)
Batch[32] - loss: 0.048239  acc: 100.0000%(16/16)
Batch[33] - loss: 0.264536  acc: 93.7500%(15/16)
Batch[34] - loss: 0.015858  acc: 100.0000%(16/16)
Batch[35] - loss: 0.158320  acc: 93.7500%(15/16)
Batch[36] - loss: 0.116990  acc: 93.7500%(15/16)
Batch[37] - loss: 0.013695  acc: 100.0000%(16/16)
Batch[38] - loss: 0.025189  acc: 100.0000%(16/16)
Batch[39] - loss: 0.020864  acc: 100.0000%(16/16)
Batch[40] - loss: 0.004816  acc: 100.0000%(16/16)
Batch[41] - loss: 0.049847  acc: 100.0000%(16/16)
Batch[42] - loss: 0.020556  acc: 100.0000%(16/16)
Batch[43] - loss: 0.011163  acc: 100.0000%(16/16)
Batch[44] - loss: 0.114912  acc: 93.7500%(15/16)
Batch[45] - loss: 0.007246  acc: 100.0000%(16/16)
Batch[46] - loss: 0.025559  acc: 100.0000%(16/16)
Batch[47] - loss: 0.032995  acc: 100.0000%(16/16)
Batch[48] - loss: 0.066206  acc: 100.0000%(16/16)
Batch[49] - loss: 0.168787  acc: 87.5000%(14/16)
Batch[50] - loss: 0.021164  acc: 100.0000%(16/16)
Batch[51] - loss: 0.038348  acc: 100.0000%(16/16)
Batch[52] - loss: 0.015916  acc: 100.0000%(16/16)
Batch[53] - loss: 0.019651  acc: 100.0000%(16/16)
Batch[54] - loss: 0.052357  acc: 100.0000%(16/16)
Batch[55] - loss: 0.021345  acc: 100.0000%(16/16)
Batch[56] - loss: 0.020904  acc: 100.0000%(16/16)
Batch[57] - loss: 0.206554  acc: 93.7500%(15/16)
Batch[58] - loss: 0.143759  acc: 93.7500%(15/16)
Batch[59] - loss: 0.069913  acc: 100.0000%(16/16)
Batch[60] - loss: 0.094377  acc: 100.0000%(16/16)
Batch[61] - loss: 0.015710  acc: 100.0000%(16/16)
Batch[62] - loss: 0.128994  acc: 100.0000%(13/13)
Average loss:0.073088 average acc:97.718262%
              precision    recall  f1-score   support

          NR    0.91429   0.84211   0.87671        38
          FR    0.87097   0.72973   0.79412        37
          UR    0.64583   0.83784   0.72941        37
          TR    0.88571   0.83784   0.86111        37

    accuracy                        0.81208       149
   macro avg    0.82920   0.81188   0.81534       149
weighted avg    0.82977   0.81208   0.81575       149


Epoch  22 / 30
Batch[0] - loss: 0.011254  acc: 100.0000%(16/16)
Batch[1] - loss: 0.033152  acc: 100.0000%(16/16)
Batch[2] - loss: 0.127071  acc: 100.0000%(16/16)
Batch[3] - loss: 0.009927  acc: 100.0000%(16/16)
Batch[4] - loss: 0.007565  acc: 100.0000%(16/16)
Batch[5] - loss: 0.027848  acc: 100.0000%(16/16)
Batch[6] - loss: 0.130576  acc: 93.7500%(15/16)
Batch[7] - loss: 0.004960  acc: 100.0000%(16/16)
Batch[8] - loss: 0.057687  acc: 100.0000%(16/16)
Batch[9] - loss: 0.026419  acc: 100.0000%(16/16)
Batch[10] - loss: 0.211452  acc: 93.7500%(15/16)
Batch[11] - loss: 0.006684  acc: 100.0000%(16/16)
Batch[12] - loss: 0.015634  acc: 100.0000%(16/16)
Batch[13] - loss: 0.026075  acc: 100.0000%(16/16)
Batch[14] - loss: 0.052051  acc: 100.0000%(16/16)
Batch[15] - loss: 0.048696  acc: 100.0000%(16/16)
Batch[16] - loss: 0.065783  acc: 100.0000%(16/16)
Batch[17] - loss: 0.083384  acc: 93.7500%(15/16)
Batch[18] - loss: 0.013337  acc: 100.0000%(16/16)
Batch[19] - loss: 0.035397  acc: 100.0000%(16/16)
Batch[20] - loss: 0.250206  acc: 93.7500%(15/16)
Batch[21] - loss: 0.238760  acc: 93.7500%(15/16)
Batch[22] - loss: 0.005664  acc: 100.0000%(16/16)
Batch[23] - loss: 0.058731  acc: 93.7500%(15/16)
Batch[24] - loss: 0.075373  acc: 100.0000%(16/16)
Batch[25] - loss: 0.012403  acc: 100.0000%(16/16)
Batch[26] - loss: 0.037207  acc: 100.0000%(16/16)
Batch[27] - loss: 0.283575  acc: 93.7500%(15/16)
Batch[28] - loss: 0.026795  acc: 100.0000%(16/16)
Batch[29] - loss: 0.009551  acc: 100.0000%(16/16)
Batch[30] - loss: 0.010739  acc: 100.0000%(16/16)
Batch[31] - loss: 0.058317  acc: 100.0000%(16/16)
Batch[32] - loss: 0.073044  acc: 100.0000%(16/16)
Batch[33] - loss: 0.004332  acc: 100.0000%(16/16)
Batch[34] - loss: 0.004923  acc: 100.0000%(16/16)
Batch[35] - loss: 0.002488  acc: 100.0000%(16/16)
Batch[36] - loss: 0.041470  acc: 100.0000%(16/16)
Batch[37] - loss: 0.056346  acc: 100.0000%(16/16)
Batch[38] - loss: 0.057565  acc: 100.0000%(16/16)
Batch[39] - loss: 0.004949  acc: 100.0000%(16/16)
Batch[40] - loss: 0.026784  acc: 100.0000%(16/16)
Batch[41] - loss: 0.072609  acc: 100.0000%(16/16)
Batch[42] - loss: 0.027547  acc: 100.0000%(16/16)
Batch[43] - loss: 0.006108  acc: 100.0000%(16/16)
Batch[44] - loss: 0.033074  acc: 100.0000%(16/16)
Batch[45] - loss: 0.010437  acc: 100.0000%(16/16)
Batch[46] - loss: 0.009408  acc: 100.0000%(16/16)
Batch[47] - loss: 0.111736  acc: 93.7500%(15/16)
Batch[48] - loss: 0.024484  acc: 100.0000%(16/16)
Batch[49] - loss: 0.009337  acc: 100.0000%(16/16)
Batch[50] - loss: 0.021811  acc: 100.0000%(16/16)
Batch[51] - loss: 0.005612  acc: 100.0000%(16/16)
Batch[52] - loss: 0.008141  acc: 100.0000%(16/16)
Batch[53] - loss: 0.414581  acc: 87.5000%(14/16)
Batch[54] - loss: 0.252283  acc: 87.5000%(14/16)
Batch[55] - loss: 0.015693  acc: 100.0000%(16/16)
Batch[56] - loss: 0.006651  acc: 100.0000%(16/16)
Batch[57] - loss: 0.027886  acc: 100.0000%(16/16)
Batch[58] - loss: 0.008875  acc: 100.0000%(16/16)
Batch[59] - loss: 0.055922  acc: 93.7500%(15/16)
Batch[60] - loss: 0.036070  acc: 100.0000%(16/16)
Batch[61] - loss: 0.045126  acc: 100.0000%(16/16)
Batch[62] - loss: 0.017910  acc: 100.0000%(13/13)
Average loss:0.056436 average acc:98.710320%
              precision    recall  f1-score   support

          NR    0.94286   0.86842   0.90411        38
          FR    0.87097   0.72973   0.79412        37
          UR    0.66000   0.89189   0.75862        37
          TR    0.90909   0.81081   0.85714        37

    accuracy                        0.82550       149
   macro avg    0.84573   0.82521   0.82850       149
weighted avg    0.84638   0.82550   0.82901       149

Val set acc: 0.825503355704698
Best val set acc: 0.825503355704698
save model!!!

Epoch  23 / 30
Batch[0] - loss: 0.017793  acc: 100.0000%(16/16)
Batch[1] - loss: 0.013469  acc: 100.0000%(16/16)
Batch[2] - loss: 0.003714  acc: 100.0000%(16/16)
Batch[3] - loss: 0.054073  acc: 100.0000%(16/16)
Batch[4] - loss: 0.070766  acc: 100.0000%(16/16)
Batch[5] - loss: 0.043679  acc: 100.0000%(16/16)
Batch[6] - loss: 0.034900  acc: 100.0000%(16/16)
Batch[7] - loss: 0.028319  acc: 100.0000%(16/16)
Batch[8] - loss: 0.015121  acc: 100.0000%(16/16)
Batch[9] - loss: 0.218670  acc: 93.7500%(15/16)
Batch[10] - loss: 0.064580  acc: 100.0000%(16/16)
Batch[11] - loss: 0.084324  acc: 93.7500%(15/16)
Batch[12] - loss: 0.025800  acc: 100.0000%(16/16)
Batch[13] - loss: 0.040531  acc: 100.0000%(16/16)
Batch[14] - loss: 0.006107  acc: 100.0000%(16/16)
Batch[15] - loss: 0.044652  acc: 100.0000%(16/16)
Batch[16] - loss: 0.200015  acc: 93.7500%(15/16)
Batch[17] - loss: 0.005076  acc: 100.0000%(16/16)
Batch[18] - loss: 0.030497  acc: 100.0000%(16/16)
Batch[19] - loss: 0.079522  acc: 93.7500%(15/16)
Batch[20] - loss: 0.019863  acc: 100.0000%(16/16)
Batch[21] - loss: 0.024767  acc: 100.0000%(16/16)
Batch[22] - loss: 0.086176  acc: 100.0000%(16/16)
Batch[23] - loss: 0.004785  acc: 100.0000%(16/16)
Batch[24] - loss: 0.013758  acc: 100.0000%(16/16)
Batch[25] - loss: 0.006810  acc: 100.0000%(16/16)
Batch[26] - loss: 0.027161  acc: 100.0000%(16/16)
Batch[27] - loss: 0.020785  acc: 100.0000%(16/16)
Batch[28] - loss: 0.011406  acc: 100.0000%(16/16)
Batch[29] - loss: 0.023152  acc: 100.0000%(16/16)
Batch[30] - loss: 0.016774  acc: 100.0000%(16/16)
Batch[31] - loss: 0.051068  acc: 100.0000%(16/16)
Batch[32] - loss: 0.012251  acc: 100.0000%(16/16)
Batch[33] - loss: 0.024450  acc: 100.0000%(16/16)
Batch[34] - loss: 0.042176  acc: 100.0000%(16/16)
Batch[35] - loss: 0.011315  acc: 100.0000%(16/16)
Batch[36] - loss: 0.041949  acc: 100.0000%(16/16)
Batch[37] - loss: 0.021081  acc: 100.0000%(16/16)
Batch[38] - loss: 0.016287  acc: 100.0000%(16/16)
Batch[39] - loss: 0.058544  acc: 100.0000%(16/16)
Batch[40] - loss: 0.014638  acc: 100.0000%(16/16)
Batch[41] - loss: 0.516573  acc: 87.5000%(14/16)
Batch[42] - loss: 0.010443  acc: 100.0000%(16/16)
Batch[43] - loss: 0.092909  acc: 93.7500%(15/16)
Batch[44] - loss: 0.005507  acc: 100.0000%(16/16)
Batch[45] - loss: 0.182439  acc: 93.7500%(15/16)
Batch[46] - loss: 0.083688  acc: 100.0000%(16/16)
Batch[47] - loss: 0.015099  acc: 100.0000%(16/16)
Batch[48] - loss: 0.067926  acc: 93.7500%(15/16)
Batch[49] - loss: 0.020858  acc: 100.0000%(16/16)
Batch[50] - loss: 0.003736  acc: 100.0000%(16/16)
Batch[51] - loss: 0.191077  acc: 93.7500%(15/16)
Batch[52] - loss: 0.409299  acc: 93.7500%(15/16)
Batch[53] - loss: 0.062214  acc: 100.0000%(16/16)
Batch[54] - loss: 0.012499  acc: 100.0000%(16/16)
Batch[55] - loss: 0.006452  acc: 100.0000%(16/16)
Batch[56] - loss: 0.236565  acc: 93.7500%(15/16)
Batch[57] - loss: 0.020923  acc: 100.0000%(16/16)
Batch[58] - loss: 0.099231  acc: 93.7500%(15/16)
Batch[59] - loss: 0.016406  acc: 100.0000%(16/16)
Batch[60] - loss: 0.132657  acc: 87.5000%(14/16)
Batch[61] - loss: 0.083204  acc: 93.7500%(15/16)
Batch[62] - loss: 0.065106  acc: 100.0000%(13/13)
Average loss:0.062946 average acc:98.412704%
              precision    recall  f1-score   support

          NR    0.89189   0.86842   0.88000        38
          FR    0.89655   0.70270   0.78788        37
          UR    0.64706   0.89189   0.75000        37
          TR    0.93750   0.81081   0.86957        37

    accuracy                        0.81879       149
   macro avg    0.84325   0.81846   0.82186       149
weighted avg    0.84358   0.81879   0.82225       149


Epoch  24 / 30
Batch[0] - loss: 0.017717  acc: 100.0000%(16/16)
Batch[1] - loss: 0.010801  acc: 100.0000%(16/16)
Batch[2] - loss: 0.078486  acc: 100.0000%(16/16)
Batch[3] - loss: 0.014438  acc: 100.0000%(16/16)
Batch[4] - loss: 0.063005  acc: 100.0000%(16/16)
Batch[5] - loss: 0.027824  acc: 100.0000%(16/16)
Batch[6] - loss: 0.001004  acc: 100.0000%(16/16)
Batch[7] - loss: 0.067845  acc: 100.0000%(16/16)
Batch[8] - loss: 0.014797  acc: 100.0000%(16/16)
Batch[9] - loss: 0.053611  acc: 100.0000%(16/16)
Batch[10] - loss: 0.036097  acc: 100.0000%(16/16)
Batch[11] - loss: 0.012219  acc: 100.0000%(16/16)
Batch[12] - loss: 0.037115  acc: 100.0000%(16/16)
Batch[13] - loss: 0.031898  acc: 100.0000%(16/16)
Batch[14] - loss: 0.010301  acc: 100.0000%(16/16)
Batch[15] - loss: 0.026173  acc: 100.0000%(16/16)
Batch[16] - loss: 0.015717  acc: 100.0000%(16/16)
Batch[17] - loss: 0.018237  acc: 100.0000%(16/16)
Batch[18] - loss: 0.379026  acc: 93.7500%(15/16)
Batch[19] - loss: 0.075907  acc: 100.0000%(16/16)
Batch[20] - loss: 0.064217  acc: 100.0000%(16/16)
Batch[21] - loss: 0.339050  acc: 93.7500%(15/16)
Batch[22] - loss: 0.008325  acc: 100.0000%(16/16)
Batch[23] - loss: 0.004879  acc: 100.0000%(16/16)
Batch[24] - loss: 0.004172  acc: 100.0000%(16/16)
Batch[25] - loss: 0.017266  acc: 100.0000%(16/16)
Batch[26] - loss: 0.039504  acc: 100.0000%(16/16)
Batch[27] - loss: 0.008511  acc: 100.0000%(16/16)
Batch[28] - loss: 0.027003  acc: 100.0000%(16/16)
Batch[29] - loss: 0.218084  acc: 93.7500%(15/16)
Batch[30] - loss: 0.004494  acc: 100.0000%(16/16)
Batch[31] - loss: 0.078671  acc: 100.0000%(16/16)
Batch[32] - loss: 0.031299  acc: 100.0000%(16/16)
Batch[33] - loss: 0.021456  acc: 100.0000%(16/16)
Batch[34] - loss: 0.005248  acc: 100.0000%(16/16)
Batch[35] - loss: 0.003038  acc: 100.0000%(16/16)
Batch[36] - loss: 0.089443  acc: 93.7500%(15/16)
Batch[37] - loss: 0.038792  acc: 100.0000%(16/16)
Batch[38] - loss: 0.014436  acc: 100.0000%(16/16)
Batch[39] - loss: 0.011931  acc: 100.0000%(16/16)
Batch[40] - loss: 0.032686  acc: 100.0000%(16/16)
Batch[41] - loss: 0.100159  acc: 93.7500%(15/16)
Batch[42] - loss: 0.011608  acc: 100.0000%(16/16)
Batch[43] - loss: 0.007206  acc: 100.0000%(16/16)
Batch[44] - loss: 0.017156  acc: 100.0000%(16/16)
Batch[45] - loss: 0.007527  acc: 100.0000%(16/16)
Batch[46] - loss: 0.066078  acc: 100.0000%(16/16)
Batch[47] - loss: 0.009132  acc: 100.0000%(16/16)
Batch[48] - loss: 0.021442  acc: 100.0000%(16/16)
Batch[49] - loss: 0.011551  acc: 100.0000%(16/16)
Batch[50] - loss: 0.031847  acc: 100.0000%(16/16)
Batch[51] - loss: 0.242875  acc: 93.7500%(15/16)
Batch[52] - loss: 0.030744  acc: 100.0000%(16/16)
Batch[53] - loss: 0.014623  acc: 100.0000%(16/16)
Batch[54] - loss: 0.023778  acc: 100.0000%(16/16)
Batch[55] - loss: 0.050956  acc: 100.0000%(16/16)
Batch[56] - loss: 0.008881  acc: 100.0000%(16/16)
Batch[57] - loss: 0.019419  acc: 100.0000%(16/16)
Batch[58] - loss: 0.048780  acc: 100.0000%(16/16)
Batch[59] - loss: 0.013805  acc: 100.0000%(16/16)
Batch[60] - loss: 0.020793  acc: 100.0000%(16/16)
Batch[61] - loss: 0.054206  acc: 100.0000%(16/16)
Batch[62] - loss: 0.335335  acc: 92.3077%(12/13)
Average loss:0.050835 average acc:99.282669%
              precision    recall  f1-score   support

          NR    0.89189   0.86842   0.88000        38
          FR    0.90000   0.72973   0.80597        37
          UR    0.63830   0.81081   0.71429        37
          TR    0.85714   0.81081   0.83333        37

    accuracy                        0.80537       149
   macro avg    0.82183   0.80494   0.80840       149
weighted avg    0.82230   0.80537   0.80888       149


Epoch  25 / 30
Batch[0] - loss: 0.011654  acc: 100.0000%(16/16)
Batch[1] - loss: 0.003719  acc: 100.0000%(16/16)
Batch[2] - loss: 0.102720  acc: 93.7500%(15/16)
Batch[3] - loss: 0.044562  acc: 100.0000%(16/16)
Batch[4] - loss: 0.013939  acc: 100.0000%(16/16)
Batch[5] - loss: 0.010113  acc: 100.0000%(16/16)
Batch[6] - loss: 0.047338  acc: 100.0000%(16/16)
Batch[7] - loss: 0.003487  acc: 100.0000%(16/16)
Batch[8] - loss: 0.019036  acc: 100.0000%(16/16)
Batch[9] - loss: 0.001298  acc: 100.0000%(16/16)
Batch[10] - loss: 0.004382  acc: 100.0000%(16/16)
Batch[11] - loss: 0.011977  acc: 100.0000%(16/16)
Batch[12] - loss: 0.011206  acc: 100.0000%(16/16)
Batch[13] - loss: 0.096472  acc: 93.7500%(15/16)
Batch[14] - loss: 0.006544  acc: 100.0000%(16/16)
Batch[15] - loss: 0.034700  acc: 100.0000%(16/16)
Batch[16] - loss: 0.029451  acc: 100.0000%(16/16)
Batch[17] - loss: 0.028535  acc: 100.0000%(16/16)
Batch[18] - loss: 0.036201  acc: 100.0000%(16/16)
Batch[19] - loss: 0.022393  acc: 100.0000%(16/16)
Batch[20] - loss: 0.003380  acc: 100.0000%(16/16)
Batch[21] - loss: 0.009004  acc: 100.0000%(16/16)
Batch[22] - loss: 0.003209  acc: 100.0000%(16/16)
Batch[23] - loss: 0.073964  acc: 100.0000%(16/16)
Batch[24] - loss: 0.002755  acc: 100.0000%(16/16)
Batch[25] - loss: 0.084254  acc: 93.7500%(15/16)
Batch[26] - loss: 0.033593  acc: 100.0000%(16/16)
Batch[27] - loss: 0.061813  acc: 100.0000%(16/16)
Batch[28] - loss: 0.025955  acc: 100.0000%(16/16)
Batch[29] - loss: 0.005056  acc: 100.0000%(16/16)
Batch[30] - loss: 0.004493  acc: 100.0000%(16/16)
Batch[31] - loss: 0.053949  acc: 100.0000%(16/16)
Batch[32] - loss: 0.216686  acc: 93.7500%(15/16)
Batch[33] - loss: 0.003258  acc: 100.0000%(16/16)
Batch[34] - loss: 0.043402  acc: 100.0000%(16/16)
Batch[35] - loss: 0.002217  acc: 100.0000%(16/16)
Batch[36] - loss: 0.151244  acc: 93.7500%(15/16)
Batch[37] - loss: 0.012704  acc: 100.0000%(16/16)
Batch[38] - loss: 0.003818  acc: 100.0000%(16/16)
Batch[39] - loss: 0.003018  acc: 100.0000%(16/16)
Batch[40] - loss: 0.011534  acc: 100.0000%(16/16)
Batch[41] - loss: 0.011072  acc: 100.0000%(16/16)
Batch[42] - loss: 0.006447  acc: 100.0000%(16/16)
Batch[43] - loss: 0.018106  acc: 100.0000%(16/16)
Batch[44] - loss: 0.002328  acc: 100.0000%(16/16)
Batch[45] - loss: 0.042433  acc: 100.0000%(16/16)
Batch[46] - loss: 0.055581  acc: 100.0000%(16/16)
Batch[47] - loss: 0.094891  acc: 93.7500%(15/16)
Batch[48] - loss: 0.016749  acc: 100.0000%(16/16)
Batch[49] - loss: 0.051884  acc: 100.0000%(16/16)
Batch[50] - loss: 0.066271  acc: 100.0000%(16/16)
Batch[51] - loss: 0.009064  acc: 100.0000%(16/16)
Batch[52] - loss: 0.080843  acc: 100.0000%(16/16)
Batch[53] - loss: 0.031448  acc: 100.0000%(16/16)
Batch[54] - loss: 0.010509  acc: 100.0000%(16/16)
Batch[55] - loss: 0.030658  acc: 100.0000%(16/16)
Batch[56] - loss: 0.045093  acc: 100.0000%(16/16)
Batch[57] - loss: 0.005213  acc: 100.0000%(16/16)
Batch[58] - loss: 0.005234  acc: 100.0000%(16/16)
Batch[59] - loss: 0.012941  acc: 100.0000%(16/16)
Batch[60] - loss: 0.015442  acc: 100.0000%(16/16)
Batch[61] - loss: 0.004275  acc: 100.0000%(16/16)
Batch[62] - loss: 0.017933  acc: 100.0000%(13/13)
Average loss:0.031483 average acc:99.404770%
              precision    recall  f1-score   support

          NR    0.91429   0.84211   0.87671        38
          FR    0.87097   0.72973   0.79412        37
          UR    0.64000   0.86486   0.73563        37
          TR    0.90909   0.81081   0.85714        37

    accuracy                        0.81208       149
   macro avg    0.83359   0.81188   0.81590       149
weighted avg    0.83413   0.81208   0.81631       149


Epoch  26 / 30
Batch[0] - loss: 0.079032  acc: 93.7500%(15/16)
Batch[1] - loss: 0.012957  acc: 100.0000%(16/16)
Batch[2] - loss: 0.000917  acc: 100.0000%(16/16)
Batch[3] - loss: 0.035465  acc: 100.0000%(16/16)
Batch[4] - loss: 0.052953  acc: 100.0000%(16/16)
Batch[5] - loss: 0.007448  acc: 100.0000%(16/16)
Batch[6] - loss: 0.037926  acc: 100.0000%(16/16)
Batch[7] - loss: 0.026596  acc: 100.0000%(16/16)
Batch[8] - loss: 0.063818  acc: 100.0000%(16/16)
Batch[9] - loss: 0.022748  acc: 100.0000%(16/16)
Batch[10] - loss: 0.072337  acc: 100.0000%(16/16)
Batch[11] - loss: 0.001587  acc: 100.0000%(16/16)
Batch[12] - loss: 0.061699  acc: 93.7500%(15/16)
Batch[13] - loss: 0.007589  acc: 100.0000%(16/16)
Batch[14] - loss: 0.013724  acc: 100.0000%(16/16)
Batch[15] - loss: 0.058005  acc: 93.7500%(15/16)
Batch[16] - loss: 0.081039  acc: 100.0000%(16/16)
Batch[17] - loss: 0.017755  acc: 100.0000%(16/16)
Batch[18] - loss: 0.009958  acc: 100.0000%(16/16)
Batch[19] - loss: 0.003207  acc: 100.0000%(16/16)
Batch[20] - loss: 0.040898  acc: 100.0000%(16/16)
Batch[21] - loss: 0.007353  acc: 100.0000%(16/16)
Batch[22] - loss: 0.011377  acc: 100.0000%(16/16)
Batch[23] - loss: 0.003271  acc: 100.0000%(16/16)
Batch[24] - loss: 0.028535  acc: 100.0000%(16/16)
Batch[25] - loss: 0.020833  acc: 100.0000%(16/16)
Batch[26] - loss: 0.013369  acc: 100.0000%(16/16)
Batch[27] - loss: 0.005337  acc: 100.0000%(16/16)
Batch[28] - loss: 0.000865  acc: 100.0000%(16/16)
Batch[29] - loss: 0.059288  acc: 93.7500%(15/16)
Batch[30] - loss: 0.025297  acc: 100.0000%(16/16)
Batch[31] - loss: 0.025249  acc: 100.0000%(16/16)
Batch[32] - loss: 0.012611  acc: 100.0000%(16/16)
Batch[33] - loss: 0.048511  acc: 100.0000%(16/16)
Batch[34] - loss: 0.011471  acc: 100.0000%(16/16)
Batch[35] - loss: 0.055401  acc: 100.0000%(16/16)
Batch[36] - loss: 0.116768  acc: 93.7500%(15/16)
Batch[37] - loss: 0.177486  acc: 93.7500%(15/16)
Batch[38] - loss: 0.004963  acc: 100.0000%(16/16)
Batch[39] - loss: 0.020450  acc: 100.0000%(16/16)
Batch[40] - loss: 0.073245  acc: 100.0000%(16/16)
Batch[41] - loss: 0.064506  acc: 100.0000%(16/16)
Batch[42] - loss: 0.041591  acc: 100.0000%(16/16)
Batch[43] - loss: 0.019547  acc: 100.0000%(16/16)
Batch[44] - loss: 0.131014  acc: 93.7500%(15/16)
Batch[45] - loss: 0.002382  acc: 100.0000%(16/16)
Batch[46] - loss: 0.018356  acc: 100.0000%(16/16)
Batch[47] - loss: 0.020214  acc: 100.0000%(16/16)
Batch[48] - loss: 0.024045  acc: 100.0000%(16/16)
Batch[49] - loss: 0.017136  acc: 100.0000%(16/16)
Batch[50] - loss: 0.005567  acc: 100.0000%(16/16)
Batch[51] - loss: 0.206966  acc: 93.7500%(15/16)
Batch[52] - loss: 0.012016  acc: 100.0000%(16/16)
Batch[53] - loss: 0.063952  acc: 100.0000%(16/16)
Batch[54] - loss: 0.012497  acc: 100.0000%(16/16)
Batch[55] - loss: 0.007664  acc: 100.0000%(16/16)
Batch[56] - loss: 0.004227  acc: 100.0000%(16/16)
Batch[57] - loss: 0.015286  acc: 100.0000%(16/16)
Batch[58] - loss: 0.009376  acc: 100.0000%(16/16)
Batch[59] - loss: 0.042731  acc: 100.0000%(16/16)
Batch[60] - loss: 0.001699  acc: 100.0000%(16/16)
Batch[61] - loss: 0.012203  acc: 100.0000%(16/16)
Batch[62] - loss: 0.002826  acc: 100.0000%(13/13)
Average loss:0.034367 average acc:99.206352%
              precision    recall  f1-score   support

          NR    0.82500   0.86842   0.84615        38
          FR    0.86667   0.70270   0.77612        37
          UR    0.63830   0.81081   0.71429        37
          TR    0.93750   0.81081   0.86957        37

    accuracy                        0.79866       149
   macro avg    0.81687   0.79819   0.80153       149
weighted avg    0.81692   0.79866   0.80183       149


Epoch  27 / 30
Batch[0] - loss: 0.008519  acc: 100.0000%(16/16)
Batch[1] - loss: 0.018943  acc: 100.0000%(16/16)
Batch[2] - loss: 0.004817  acc: 100.0000%(16/16)
Batch[3] - loss: 0.014278  acc: 100.0000%(16/16)
Batch[4] - loss: 0.023200  acc: 100.0000%(16/16)
Batch[5] - loss: 0.030304  acc: 100.0000%(16/16)
Batch[6] - loss: 0.022767  acc: 100.0000%(16/16)
Batch[7] - loss: 0.032381  acc: 100.0000%(16/16)
Batch[8] - loss: 0.036644  acc: 100.0000%(16/16)
Batch[9] - loss: 0.023155  acc: 100.0000%(16/16)
Batch[10] - loss: 0.008321  acc: 100.0000%(16/16)
Batch[11] - loss: 0.019204  acc: 100.0000%(16/16)
Batch[12] - loss: 0.002993  acc: 100.0000%(16/16)
Batch[13] - loss: 0.015734  acc: 100.0000%(16/16)
Batch[14] - loss: 0.007618  acc: 100.0000%(16/16)
Batch[15] - loss: 0.013273  acc: 100.0000%(16/16)
Batch[16] - loss: 0.039993  acc: 100.0000%(16/16)
Batch[17] - loss: 0.004730  acc: 100.0000%(16/16)
Batch[18] - loss: 0.005085  acc: 100.0000%(16/16)
Batch[19] - loss: 0.016490  acc: 100.0000%(16/16)
Batch[20] - loss: 0.017054  acc: 100.0000%(16/16)
Batch[21] - loss: 0.031012  acc: 100.0000%(16/16)
Batch[22] - loss: 0.014217  acc: 100.0000%(16/16)
Batch[23] - loss: 0.002068  acc: 100.0000%(16/16)
Batch[24] - loss: 0.054265  acc: 100.0000%(16/16)
Batch[25] - loss: 0.026469  acc: 100.0000%(16/16)
Batch[26] - loss: 0.038387  acc: 100.0000%(16/16)
Batch[27] - loss: 0.101712  acc: 93.7500%(15/16)
Batch[28] - loss: 0.044917  acc: 100.0000%(16/16)
Batch[29] - loss: 0.098000  acc: 93.7500%(15/16)
Batch[30] - loss: 0.032004  acc: 100.0000%(16/16)
Batch[31] - loss: 0.045964  acc: 100.0000%(16/16)
Batch[32] - loss: 0.003678  acc: 100.0000%(16/16)
Batch[33] - loss: 0.050231  acc: 100.0000%(16/16)
Batch[34] - loss: 0.042272  acc: 100.0000%(16/16)
Batch[35] - loss: 0.012424  acc: 100.0000%(16/16)
Batch[36] - loss: 0.035088  acc: 100.0000%(16/16)
Batch[37] - loss: 0.036740  acc: 100.0000%(16/16)
Batch[38] - loss: 0.003832  acc: 100.0000%(16/16)
Batch[39] - loss: 0.007337  acc: 100.0000%(16/16)
Batch[40] - loss: 0.005872  acc: 100.0000%(16/16)
Batch[41] - loss: 0.031894  acc: 100.0000%(16/16)
Batch[42] - loss: 0.011104  acc: 100.0000%(16/16)
Batch[43] - loss: 0.015472  acc: 100.0000%(16/16)
Batch[44] - loss: 0.037909  acc: 100.0000%(16/16)
Batch[45] - loss: 0.023636  acc: 100.0000%(16/16)
Batch[46] - loss: 0.022057  acc: 100.0000%(16/16)
Batch[47] - loss: 0.011137  acc: 100.0000%(16/16)
Batch[48] - loss: 0.036111  acc: 100.0000%(16/16)
Batch[49] - loss: 0.006025  acc: 100.0000%(16/16)
Batch[50] - loss: 0.010631  acc: 100.0000%(16/16)
Batch[51] - loss: 0.030166  acc: 100.0000%(16/16)
Batch[52] - loss: 0.127884  acc: 93.7500%(15/16)
Batch[53] - loss: 0.003830  acc: 100.0000%(16/16)
Batch[54] - loss: 0.232104  acc: 93.7500%(15/16)
Batch[55] - loss: 0.003431  acc: 100.0000%(16/16)
Batch[56] - loss: 0.071082  acc: 93.7500%(15/16)
Batch[57] - loss: 0.002985  acc: 100.0000%(16/16)
Batch[58] - loss: 0.011933  acc: 100.0000%(16/16)
Batch[59] - loss: 0.016576  acc: 100.0000%(16/16)
Batch[60] - loss: 0.006478  acc: 100.0000%(16/16)
Batch[61] - loss: 0.061962  acc: 100.0000%(16/16)
Batch[62] - loss: 0.004025  acc: 100.0000%(13/13)
Average loss:0.029054 average acc:99.503975%
Reload the best model...
6.25e-05
              precision    recall  f1-score   support

          NR    0.94286   0.86842   0.90411        38
          FR    0.87097   0.72973   0.79412        37
          UR    0.66000   0.89189   0.75862        37
          TR    0.90909   0.81081   0.85714        37

    accuracy                        0.82550       149
   macro avg    0.84573   0.82521   0.82850       149
weighted avg    0.84638   0.82550   0.82901       149


Epoch  28 / 30
Batch[0] - loss: 0.013107  acc: 100.0000%(16/16)
Batch[1] - loss: 0.044275  acc: 100.0000%(16/16)
Batch[2] - loss: 0.007216  acc: 100.0000%(16/16)
Batch[3] - loss: 0.451518  acc: 93.7500%(15/16)
Batch[4] - loss: 0.013186  acc: 100.0000%(16/16)
Batch[5] - loss: 0.055692  acc: 100.0000%(16/16)
Batch[6] - loss: 0.060811  acc: 100.0000%(16/16)
Batch[7] - loss: 0.053961  acc: 100.0000%(16/16)
Batch[8] - loss: 0.141944  acc: 93.7500%(15/16)
Batch[9] - loss: 0.324459  acc: 87.5000%(14/16)
Batch[10] - loss: 0.075407  acc: 100.0000%(16/16)
Batch[11] - loss: 0.027937  acc: 100.0000%(16/16)
Batch[12] - loss: 0.007968  acc: 100.0000%(16/16)
Batch[13] - loss: 0.014272  acc: 100.0000%(16/16)
Batch[14] - loss: 0.062395  acc: 100.0000%(16/16)
Batch[15] - loss: 0.064394  acc: 100.0000%(16/16)
Batch[16] - loss: 0.013924  acc: 100.0000%(16/16)
Batch[17] - loss: 0.010760  acc: 100.0000%(16/16)
Batch[18] - loss: 0.027701  acc: 100.0000%(16/16)
Batch[19] - loss: 0.006950  acc: 100.0000%(16/16)
Batch[20] - loss: 0.026952  acc: 100.0000%(16/16)
Batch[21] - loss: 0.010822  acc: 100.0000%(16/16)
Batch[22] - loss: 0.014947  acc: 100.0000%(16/16)
Batch[23] - loss: 0.009493  acc: 100.0000%(16/16)
Batch[24] - loss: 0.047576  acc: 100.0000%(16/16)
Batch[25] - loss: 0.132624  acc: 93.7500%(15/16)
Batch[26] - loss: 0.020761  acc: 100.0000%(16/16)
Batch[27] - loss: 0.048556  acc: 100.0000%(16/16)
Batch[28] - loss: 0.038520  acc: 100.0000%(16/16)
Batch[29] - loss: 0.020128  acc: 100.0000%(16/16)
Batch[30] - loss: 0.034626  acc: 100.0000%(16/16)
Batch[31] - loss: 0.032794  acc: 100.0000%(16/16)
Batch[32] - loss: 0.007401  acc: 100.0000%(16/16)
Batch[33] - loss: 0.069359  acc: 100.0000%(16/16)
Batch[34] - loss: 0.073126  acc: 100.0000%(16/16)
Batch[35] - loss: 0.011478  acc: 100.0000%(16/16)
Batch[36] - loss: 0.098792  acc: 93.7500%(15/16)
Batch[37] - loss: 0.047421  acc: 100.0000%(16/16)
Batch[38] - loss: 0.032554  acc: 100.0000%(16/16)
Batch[39] - loss: 0.030225  acc: 100.0000%(16/16)
Batch[40] - loss: 0.015617  acc: 100.0000%(16/16)
Batch[41] - loss: 0.004085  acc: 100.0000%(16/16)
Batch[42] - loss: 0.089098  acc: 93.7500%(15/16)
Batch[43] - loss: 0.013654  acc: 100.0000%(16/16)
Batch[44] - loss: 0.032550  acc: 100.0000%(16/16)
Batch[45] - loss: 0.037891  acc: 100.0000%(16/16)
Batch[46] - loss: 0.177469  acc: 87.5000%(14/16)
Batch[47] - loss: 0.189971  acc: 87.5000%(14/16)
Batch[48] - loss: 0.036042  acc: 100.0000%(16/16)
Batch[49] - loss: 0.068667  acc: 93.7500%(15/16)
Batch[50] - loss: 0.028748  acc: 100.0000%(16/16)
Batch[51] - loss: 0.021830  acc: 100.0000%(16/16)
Batch[52] - loss: 0.008847  acc: 100.0000%(16/16)
Batch[53] - loss: 0.039477  acc: 100.0000%(16/16)
Batch[54] - loss: 0.183293  acc: 93.7500%(15/16)
Batch[55] - loss: 0.155059  acc: 93.7500%(15/16)
Batch[56] - loss: 0.291460  acc: 93.7500%(15/16)
Batch[57] - loss: 0.299456  acc: 93.7500%(15/16)
Batch[58] - loss: 0.089644  acc: 100.0000%(16/16)
Batch[59] - loss: 0.009827  acc: 100.0000%(16/16)
Batch[60] - loss: 0.035126  acc: 100.0000%(16/16)
Batch[61] - loss: 0.180302  acc: 93.7500%(15/16)
Batch[62] - loss: 0.003014  acc: 100.0000%(13/13)
Average loss:0.068209 average acc:98.313499%
              precision    recall  f1-score   support

          NR    0.94118   0.84211   0.88889        38
          FR    0.84375   0.72973   0.78261        37
          UR    0.64000   0.86486   0.73563        37
          TR    0.90909   0.81081   0.85714        37

    accuracy                        0.81208       149
   macro avg    0.83350   0.81188   0.81607       149
weighted avg    0.83423   0.81208   0.81656       149


Epoch  29 / 30
Batch[0] - loss: 0.053781  acc: 100.0000%(16/16)
Batch[1] - loss: 0.021683  acc: 100.0000%(16/16)
Batch[2] - loss: 0.003683  acc: 100.0000%(16/16)
Batch[3] - loss: 0.015706  acc: 100.0000%(16/16)
Batch[4] - loss: 0.278982  acc: 93.7500%(15/16)
Batch[5] - loss: 0.024924  acc: 100.0000%(16/16)
Batch[6] - loss: 0.017621  acc: 100.0000%(16/16)
Batch[7] - loss: 0.004086  acc: 100.0000%(16/16)
Batch[8] - loss: 0.096815  acc: 93.7500%(15/16)
Batch[9] - loss: 0.098922  acc: 93.7500%(15/16)
Batch[10] - loss: 0.019716  acc: 100.0000%(16/16)
Batch[11] - loss: 0.007355  acc: 100.0000%(16/16)
Batch[12] - loss: 0.032711  acc: 100.0000%(16/16)
Batch[13] - loss: 0.031817  acc: 100.0000%(16/16)
Batch[14] - loss: 0.025256  acc: 100.0000%(16/16)
Batch[15] - loss: 0.007426  acc: 100.0000%(16/16)
Batch[16] - loss: 0.111695  acc: 100.0000%(16/16)
Batch[17] - loss: 0.047607  acc: 100.0000%(16/16)
Batch[18] - loss: 0.026378  acc: 100.0000%(16/16)
Batch[19] - loss: 0.018412  acc: 100.0000%(16/16)
Batch[20] - loss: 0.017914  acc: 100.0000%(16/16)
Batch[21] - loss: 0.012731  acc: 100.0000%(16/16)
Batch[22] - loss: 0.025944  acc: 100.0000%(16/16)
Batch[23] - loss: 0.290507  acc: 93.7500%(15/16)
Batch[24] - loss: 0.017434  acc: 100.0000%(16/16)
Batch[25] - loss: 0.003090  acc: 100.0000%(16/16)
Batch[26] - loss: 0.095951  acc: 100.0000%(16/16)
Batch[27] - loss: 0.039608  acc: 100.0000%(16/16)
Batch[28] - loss: 0.056767  acc: 100.0000%(16/16)
Batch[29] - loss: 0.032045  acc: 100.0000%(16/16)
Batch[30] - loss: 0.131655  acc: 93.7500%(15/16)
Batch[31] - loss: 0.161887  acc: 93.7500%(15/16)
Batch[32] - loss: 0.059134  acc: 100.0000%(16/16)
Batch[33] - loss: 0.021257  acc: 100.0000%(16/16)
Batch[34] - loss: 0.012767  acc: 100.0000%(16/16)
Batch[35] - loss: 0.004746  acc: 100.0000%(16/16)
Batch[36] - loss: 0.018793  acc: 100.0000%(16/16)
Batch[37] - loss: 0.017802  acc: 100.0000%(16/16)
Batch[38] - loss: 0.248864  acc: 93.7500%(15/16)
Batch[39] - loss: 0.063606  acc: 100.0000%(16/16)
Batch[40] - loss: 0.008733  acc: 100.0000%(16/16)
Batch[41] - loss: 0.026294  acc: 100.0000%(16/16)
Batch[42] - loss: 0.014324  acc: 100.0000%(16/16)
Batch[43] - loss: 0.038075  acc: 100.0000%(16/16)
Batch[44] - loss: 0.026282  acc: 100.0000%(16/16)
Batch[45] - loss: 0.241910  acc: 93.7500%(15/16)
Batch[46] - loss: 0.018121  acc: 100.0000%(16/16)
Batch[47] - loss: 0.029478  acc: 100.0000%(16/16)
Batch[48] - loss: 0.051558  acc: 100.0000%(16/16)
Batch[49] - loss: 0.010411  acc: 100.0000%(16/16)
Batch[50] - loss: 0.077111  acc: 100.0000%(16/16)
Batch[51] - loss: 0.035646  acc: 100.0000%(16/16)
Batch[52] - loss: 0.007391  acc: 100.0000%(16/16)
Batch[53] - loss: 0.027352  acc: 100.0000%(16/16)
Batch[54] - loss: 0.082111  acc: 93.7500%(15/16)
Batch[55] - loss: 0.181971  acc: 93.7500%(15/16)
Batch[56] - loss: 0.015684  acc: 100.0000%(16/16)
Batch[57] - loss: 0.012876  acc: 100.0000%(16/16)
Batch[58] - loss: 0.009067  acc: 100.0000%(16/16)
Batch[59] - loss: 0.009236  acc: 100.0000%(16/16)
Batch[60] - loss: 0.002870  acc: 100.0000%(16/16)
Batch[61] - loss: 0.048374  acc: 100.0000%(16/16)
Batch[62] - loss: 0.007485  acc: 100.0000%(13/13)
Average loss:0.051737 average acc:99.007942%
              precision    recall  f1-score   support

          NR    0.94118   0.84211   0.88889        38
          FR    0.84375   0.72973   0.78261        37
          UR    0.64000   0.86486   0.73563        37
          TR    0.90909   0.81081   0.85714        37

    accuracy                        0.81208       149
   macro avg    0.83350   0.81188   0.81607       149
weighted avg    0.83423   0.81208   0.81656       149


Epoch  30 / 30
Batch[0] - loss: 0.007082  acc: 100.0000%(16/16)
Batch[1] - loss: 0.034460  acc: 100.0000%(16/16)
Batch[2] - loss: 0.080967  acc: 93.7500%(15/16)
Batch[3] - loss: 0.014867  acc: 100.0000%(16/16)
Batch[4] - loss: 0.088500  acc: 93.7500%(15/16)
Batch[5] - loss: 0.128083  acc: 93.7500%(15/16)
Batch[6] - loss: 0.026002  acc: 100.0000%(16/16)
Batch[7] - loss: 0.089564  acc: 93.7500%(15/16)
Batch[8] - loss: 0.010112  acc: 100.0000%(16/16)
Batch[9] - loss: 0.008806  acc: 100.0000%(16/16)
Batch[10] - loss: 0.033636  acc: 100.0000%(16/16)
Batch[11] - loss: 0.030709  acc: 100.0000%(16/16)
Batch[12] - loss: 0.015226  acc: 100.0000%(16/16)
Batch[13] - loss: 0.047742  acc: 100.0000%(16/16)
Batch[14] - loss: 0.003114  acc: 100.0000%(16/16)
Batch[15] - loss: 0.023884  acc: 100.0000%(16/16)
Batch[16] - loss: 0.018280  acc: 100.0000%(16/16)
Batch[17] - loss: 0.003769  acc: 100.0000%(16/16)
Batch[18] - loss: 0.003754  acc: 100.0000%(16/16)
Batch[19] - loss: 0.038991  acc: 100.0000%(16/16)
Batch[20] - loss: 0.014837  acc: 100.0000%(16/16)
Batch[21] - loss: 0.093423  acc: 100.0000%(16/16)
Batch[22] - loss: 0.076387  acc: 100.0000%(16/16)
Batch[23] - loss: 0.214246  acc: 87.5000%(14/16)
Batch[24] - loss: 0.004486  acc: 100.0000%(16/16)
Batch[25] - loss: 0.035566  acc: 100.0000%(16/16)
Batch[26] - loss: 0.078037  acc: 100.0000%(16/16)
Batch[27] - loss: 0.019973  acc: 100.0000%(16/16)
Batch[28] - loss: 0.035170  acc: 100.0000%(16/16)
Batch[29] - loss: 0.028647  acc: 100.0000%(16/16)
Batch[30] - loss: 0.019400  acc: 100.0000%(16/16)
Batch[31] - loss: 0.005793  acc: 100.0000%(16/16)
Batch[32] - loss: 0.002930  acc: 100.0000%(16/16)
Batch[33] - loss: 0.015304  acc: 100.0000%(16/16)
Batch[34] - loss: 0.164260  acc: 93.7500%(15/16)
Batch[35] - loss: 0.099735  acc: 93.7500%(15/16)
Batch[36] - loss: 0.008264  acc: 100.0000%(16/16)
Batch[37] - loss: 0.007561  acc: 100.0000%(16/16)
Batch[38] - loss: 0.004159  acc: 100.0000%(16/16)
Batch[39] - loss: 0.009474  acc: 100.0000%(16/16)
Batch[40] - loss: 0.013696  acc: 100.0000%(16/16)
Batch[41] - loss: 0.007405  acc: 100.0000%(16/16)
Batch[42] - loss: 0.100898  acc: 93.7500%(15/16)
Batch[43] - loss: 0.018099  acc: 100.0000%(16/16)
Batch[44] - loss: 0.019852  acc: 100.0000%(16/16)
Batch[45] - loss: 0.015033  acc: 100.0000%(16/16)
Batch[46] - loss: 0.035818  acc: 100.0000%(16/16)
Batch[47] - loss: 0.136717  acc: 93.7500%(15/16)
Batch[48] - loss: 0.047836  acc: 100.0000%(16/16)
Batch[49] - loss: 0.030163  acc: 100.0000%(16/16)
Batch[50] - loss: 0.037085  acc: 100.0000%(16/16)
Batch[51] - loss: 0.006662  acc: 100.0000%(16/16)
Batch[52] - loss: 0.022201  acc: 100.0000%(16/16)
Batch[53] - loss: 0.057598  acc: 93.7500%(15/16)
Batch[54] - loss: 0.170786  acc: 93.7500%(15/16)
Batch[55] - loss: 0.029400  acc: 100.0000%(16/16)
Batch[56] - loss: 0.055316  acc: 100.0000%(16/16)
Batch[57] - loss: 0.040404  acc: 100.0000%(16/16)
Batch[58] - loss: 0.016571  acc: 100.0000%(16/16)
Batch[59] - loss: 0.012534  acc: 100.0000%(16/16)
Batch[60] - loss: 0.026410  acc: 100.0000%(16/16)
Batch[61] - loss: 0.144267  acc: 93.7500%(15/16)
Batch[62] - loss: 0.058187  acc: 100.0000%(13/13)
Average loss:0.043621 average acc:98.710320%
              precision    recall  f1-score   support

          NR    0.94118   0.84211   0.88889        38
          FR    0.90000   0.72973   0.80597        37
          UR    0.63462   0.89189   0.74157        37
          TR    0.90909   0.81081   0.85714        37

    accuracy                        0.81879       149
   macro avg    0.84622   0.81863   0.82339       149
weighted avg    0.84686   0.81879   0.82383       149

================================
              precision    recall  f1-score   support

          NR      0.953     0.964     0.959        84
          FR      0.875     0.917     0.895        84
          UR      0.847     0.857     0.852        84
          TR      0.949     0.881     0.914        84

    accuracy                          0.905       336
   macro avg      0.906     0.905     0.905       336
weighted avg      0.906     0.905     0.905       336


Process finished with exit code 0
